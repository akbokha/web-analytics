{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(899, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_action = pd.read_csv('data/action_condition_meta.csv')\n",
    "#df_action = df_action.drop('user_id', 1)\n",
    "#df_action.drop_duplicates(subset = 'user_id', inplace = True)\n",
    "df_action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#created own priorityqueue class, because the standard priorityqueue blocks (can't insert element) when max.size is reached.\n",
    "#we need to have a priorityqueue where the item with lowest priority is discarded and replaced by the new item.\n",
    "#priorityqueue represents min heap \n",
    "import heapq\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "class priority_queue:\n",
    "    def __init__(self, max_size):\n",
    "        self.items = []\n",
    "        self.max = max_size\n",
    "   \n",
    "    def push(self, item, priority):\n",
    "        if len(self.items) < self.max:\n",
    "            heapq.heappush(self.items, (priority, item))\n",
    "        else:\n",
    "            heapq.heappushpop(self.items, (priority, item))\n",
    "            \n",
    "\n",
    "    def pop(self):\n",
    "        return heapq.heappop(self.items)\n",
    "\n",
    "    def get_max_item(self):\n",
    "        return self.items[0]\n",
    "    \n",
    "    def empty(self):\n",
    "        return not self.items\n",
    "    \n",
    "    def print_elements(self):\n",
    "        result = []\n",
    "        for i in self.items:\n",
    "            result.append(i)\n",
    "        return result\n",
    "       \n",
    "    def heap_sort(self):\n",
    "        return [heapq.heappop(self.items) for _ in range(len(self.items))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#def phiYule(Set):\n",
    "#implementation of Yule's Quality measure (2b)\n",
    "#old phiYule function\n",
    "#def phiYule(Set, column_name):\n",
    "    n_1 = df_action.loc[(df_action[column_name] == Set) & (df_action.action == 'view') & (df_action.condition == '1-Control')].count()[0]\n",
    "    n_2 = df_action.loc[(df_action[column_name] == Set) & (df_action.action == 'clic') & (df_action.condition == '1-Control')].count()[0]\n",
    "    n_3 = df_action.loc[(df_action[column_name] == Set) & (df_action.action == 'view') & (df_action.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    n_4 = df_action.loc[(df_action[column_name] == Set) & (df_action.action == 'clic') & (df_action.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    yuleQ = (n_1*n_4 - n_2*n_3)/(n_1*n_4-n_2*n_3)\n",
    "    return yuleQ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# implementation of refinement operator\n",
    "# returns the unique values in the column, so that we can use these values for building a description language\n",
    "# old refinement function\n",
    "# def refinement(seed):\n",
    " #   descriptions = df_action[seed].unique()\n",
    "  #  return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#desc = [('refr_source', 'Google'), ('geo_country', 'GB')]\n",
    "#df_match_condition = create_dataframe(desc)\n",
    "#constraints_2(df_match_condition)\n",
    "#df_match_condition.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#desc = [('refr_source', 'Google'), ('geo_country', 'AZ')]\n",
    "#phiYule(df_match_condition)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#old constraint refinement\n",
    "def constraints(Set):\n",
    "    total = 0\n",
    "    for column, item in Set:\n",
    "        total += df_action.loc[(df_action[column] == item)].count()[0]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbins = 3\\nlanguage = []\\nThe refinement operator. The refinement operator gets the records and chooses to which type of attribute they belong.\\nThere are 3 types of attributes:\\n    1. Numeric: Attribute with all number records\\n    2. Binary: Attribute with true or false records\\n    3. Nominal: Attribute with multiple different values in their records which are not numeric\\n    \\n1. For numeric values we sort all records which are in the description language D. After that we make equal-sized bins.\\nThe amount of bins is dependend on a predefined value. For each split point we add a description based on whether the\\nnumeric value is greater or equal or smaller or equal as the split point.\\n2. For binary records we add one description where the description is true and one description where the description is false.\\n3. For nominal values we add for each description an entry with the description and one without the description. In our case\\nwe take the first description in the list which is not equal to that description.\\ndef refinement(seed):\\n    dataframe = create_dataframe(language)\\n    unique_records = dataframe[seed].unique()\\n    if unique_records.size == 0:\\n        return language\\n    print(\"uniques: \")\\n    print(unique_records)\\n    if isNumeric(unique_records):     #Attribute is numeric\\n        language.append(-1) #TODO       \\n    elif unique_records.size == 2:    #Attribute is binary\\n        language.append((seed, True))\\n        language.append((seed, False))\\n    else:                             #Attribute is nominal\\n        for item in unique_records:\\n            language.append((seed, item))\\n            language.append((seed, unique_records[next((i for i, v in enumerate(unique_records) if v != item), -1)]))\\n    return language\\n\\nChecks whether the records are all numeric.\\n\\ndef isNumeric(unique_records):\\n    for item in unique_records:\\n        if not item.isdigit():\\n            return False\\n    return True\\n\\ncandidateQueue = list(df.columns.values) #instead of queue that is used in the paper, intialize the queue immediately\\ncandidateQueue.remove(\\'action\\') #with the headers of the dataframe\\ncandidateQueue.remove(\\'condition\\') #remove action and condition (targets) from this, because we only need the descriptors \\n\\nseed = candidateQueue.pop()\\nprint(\"seed: \" + seed)\\nprint(\"lang:\")\\nprint(language)\\nprint(refinement(seed))\\nprint(\"ref result:\")\\n\\nseed = candidateQueue.pop()\\nprint(\"seed: \" + seed)\\nprint(\"lang:\")\\nprint(language)\\nprint(refinement(seed))\\nprint(\"ref result:\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bins = 3\n",
    "language = []\n",
    "The refinement operator. The refinement operator gets the records and chooses to which type of attribute they belong.\n",
    "There are 3 types of attributes:\n",
    "    1. Numeric: Attribute with all number records\n",
    "    2. Binary: Attribute with true or false records\n",
    "    3. Nominal: Attribute with multiple different values in their records which are not numeric\n",
    "    \n",
    "1. For numeric values we sort all records which are in the description language D. After that we make equal-sized bins.\n",
    "The amount of bins is dependend on a predefined value. For each split point we add a description based on whether the\n",
    "numeric value is greater or equal or smaller or equal as the split point.\n",
    "2. For binary records we add one description where the description is true and one description where the description is false.\n",
    "3. For nominal values we add for each description an entry with the description and one without the description. In our case\n",
    "we take the first description in the list which is not equal to that description.\n",
    "def refinement(seed):\n",
    "    dataframe = create_dataframe(language)\n",
    "    unique_records = dataframe[seed].unique()\n",
    "    if unique_records.size == 0:\n",
    "        return language\n",
    "    print(\"uniques: \")\n",
    "    print(unique_records)\n",
    "    if isNumeric(unique_records):     #Attribute is numeric\n",
    "        language.append(-1) #TODO       \n",
    "    elif unique_records.size == 2:    #Attribute is binary\n",
    "        language.append((seed, True))\n",
    "        language.append((seed, False))\n",
    "    else:                             #Attribute is nominal\n",
    "        for item in unique_records:\n",
    "            language.append((seed, item))\n",
    "            language.append((seed, unique_records[next((i for i, v in enumerate(unique_records) if v != item), -1)]))\n",
    "    return language\n",
    "\n",
    "Checks whether the records are all numeric.\n",
    "\n",
    "def isNumeric(unique_records):\n",
    "    for item in unique_records:\n",
    "        if not item.isdigit():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "candidateQueue = list(df.columns.values) #instead of queue that is used in the paper, intialize the queue immediately\n",
    "candidateQueue.remove('action') #with the headers of the dataframe\n",
    "candidateQueue.remove('condition') #remove action and condition (targets) from this, because we only need the descriptors \n",
    "\n",
    "seed = candidateQueue.pop()\n",
    "print(\"seed: \" + seed)\n",
    "print(\"lang:\")\n",
    "print(language)\n",
    "print(refinement(seed))\n",
    "print(\"ref result:\")\n",
    "\n",
    "seed = candidateQueue.pop()\n",
    "print(\"seed: \" + seed)\n",
    "print(\"lang:\")\n",
    "print(language)\n",
    "print(refinement(seed))\n",
    "print(\"ref result:\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0, ('os_timezone', 'Australia')),\n",
       " (-0.27272727272727271, ('geo_country', 'GH')),\n",
       " (-1.0, ('geo_country', 'LB')),\n",
       " (nan, ('geo_country', 'MY')),\n",
       " (1.0, ('os_timezone', 'Australia'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def phiEntropy(Set): --> to be implemented!\n",
    "#descLang = set()\n",
    "descLang = []\n",
    "numberOfBins = 3 #Should be used for numeric value refinement\n",
    "'''\n",
    "The refinement operator. The refinement operator gets the records and chooses to which type of attribute they belong.\n",
    "There are 3 types of attributes:\n",
    "    1. Numeric: Attribute with all number records\n",
    "    2. Binary: Attribute with true or false records\n",
    "    3. Nominal: Attribute with multiple different values in their records which are not numeric\n",
    "    \n",
    "1. For numeric values we sort all records which are in the description language D. After that we make equal-sized bins.\n",
    "The amount of bins is dependend on a predefined value. For each split point we add a description based on whether the\n",
    "numeric value is greater or equal or smaller or equal as the split point.\n",
    "2. For binary records we add one description where the description is true and one description where the description is false.\n",
    "3. For nominal values we add for each description an entry with the description and one without the description. In our case\n",
    "we take the first description in the list which is not equal to that description.\n",
    "'''\n",
    "def refinement(seed):\n",
    "    #dataframe = create_dataframe(descLang)\n",
    "    unique_records = df_action[seed].unique()\n",
    "    if unique_records.size == 0:\n",
    "        return descLang\n",
    "    if isNumeric(unique_records):     #Attribute is numeric\n",
    "        descLang.append(-1) # TODO       \n",
    "    elif unique_records.size == 2:    #Attribute is binary\n",
    "        descLang.append((seed, True))\n",
    "        descLang.append((seed, False))\n",
    "    else:                             #Attribute is nominal\n",
    "        for item in unique_records:\n",
    "            descLang.append((seed, item))\n",
    "            descLang.append((seed, unique_records[next((i for i, v in enumerate(unique_records) if v != item), -1)]))\n",
    "    return descLang\n",
    "'''\n",
    "descList = []\n",
    "def refinement(seed):\n",
    "    descriptions = df_action[seed].unique()\n",
    "    for item in descriptions:\n",
    "        descList.append((seed, item))\n",
    "    return descList\n",
    "'''\n",
    "'''\n",
    "Checks whether a list of values is all numeric\n",
    "'''\n",
    "def isNumeric(values):\n",
    "    for item in values:\n",
    "        if not item.isdigit():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "'''\n",
    "Creates a dataframe filtered by descriptors\n",
    "'''\n",
    "def create_dataframe(Set):\n",
    "    d =df_action.copy()\n",
    "    count = 0\n",
    "    for column, item in Set:\n",
    "        if count==0:  \n",
    "            df_new = pd.merge(d, df_action.loc[(df_action[column] == item)], on=list(df_action), how='inner')\n",
    "            df_new.drop_duplicates(inplace = True)\n",
    "        else:\n",
    "            df_new = pd.merge(df_new, df_action.loc[(df_action[column] == item)], on=list(df_action), how='inner')\n",
    "            df_new.drop_duplicates(inplace = True)\n",
    "        count +=1\n",
    "    return df_new\n",
    "\n",
    "def constraints(df_matches):\n",
    "    return df_matches.shape[0] > 5\n",
    "\n",
    "def phiYule(df_matches):\n",
    "    n_1 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_2 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_3 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    n_4 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    yuleQ = (n_1*n_4 - n_2*n_3)/(n_1*n_4 + n_2*n_3)\n",
    "    return yuleQ\n",
    "\n",
    "def beam_search(d, w, q):\n",
    "    candidateQueue = list(df_action.columns.values) #instead of queue that is used in the paper, intialize the queue immediately\n",
    "    candidateQueue.remove('action') #with the headers of the dataframe\n",
    "    candidateQueue.remove('condition') #remove action and condition (targets) from this, because we only need the descriptors \n",
    "    candidateQueue.remove('user_id')\n",
    "    #pseudo-code below is in line with the paper\n",
    "    resultSet = priority_queue(q)\n",
    "    for level in range(0, d):\n",
    "        print(\"level: \"+str(level))\n",
    "        beam = priority_queue(w)\n",
    "        while (len(candidateQueue) > 0):\n",
    "            seed = candidateQueue.pop(0)\n",
    "            set_refined = list(set(refinement(seed)))\n",
    "            for desc in set_refined:\n",
    "                df_match_condition = create_dataframe([desc])\n",
    "                quality = phiYule(df_match_condition)\n",
    "                if constraints(df_match_condition):\n",
    "                    resultSet.push(desc, quality)\n",
    "                    beam.push(desc, quality)\n",
    "            while not beam.empty:\n",
    "                candidateQueue.append(priority_queue.pop(beam))\n",
    "    return resultSet\n",
    "\n",
    "result = beam_search(d=2, w=5, q=5)\n",
    "priority_queue.heap_sort(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
