{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 A&B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "from queue import Queue\n",
    "import math\n",
    "import itertools\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import random\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') #ignore warning messages from output beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>user_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>refr_source</th>\n",
       "      <th>browser_language</th>\n",
       "      <th>os_name</th>\n",
       "      <th>os_timezone</th>\n",
       "      <th>dvce_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clic</td>\n",
       "      <td>379881d5-32d7-49f4-bf5b-81fefbc5fcce</td>\n",
       "      <td>1-Control</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>greek</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clic</td>\n",
       "      <td>2a0f4218-4f62-479b-845c-109b2720e6e7</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clic</td>\n",
       "      <td>a511b6dc-2dca-455b-b5e2-bf2d224a5505</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>GB</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clic</td>\n",
       "      <td>9fb616a7-4e13-4307-ac92-0b075d7d376a</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clic</td>\n",
       "      <td>64816772-688d-4460-a591-79aa49bba0d5</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>BD</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  action                               user_id                     condition  \\\n",
       "0   clic  379881d5-32d7-49f4-bf5b-81fefbc5fcce                     1-Control   \n",
       "1   clic  2a0f4218-4f62-479b-845c-109b2720e6e7  2-Buttony-Conversion-Buttons   \n",
       "2   clic  a511b6dc-2dca-455b-b5e2-bf2d224a5505  2-Buttony-Conversion-Buttons   \n",
       "3   clic  9fb616a7-4e13-4307-ac92-0b075d7d376a  2-Buttony-Conversion-Buttons   \n",
       "4   clic  64816772-688d-4460-a591-79aa49bba0d5  2-Buttony-Conversion-Buttons   \n",
       "\n",
       "  geo_country refr_source browser_language  os_name os_timezone dvce_type  \n",
       "0          FI      Google            greek  Android      Europe    Mobile  \n",
       "1          AU      Google          english      iOS   Australia    Mobile  \n",
       "2          GB      Google          english  Android      Europe    Mobile  \n",
       "3          FI      Google          english      iOS      Europe    Mobile  \n",
       "4          BD      Google          english  Android        Asia    Mobile  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_action = pd.read_csv('data/action_condition_meta.csv')\n",
    "df_action.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a - Beam Search implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We created our own priority queue class, because the standard priority queue blocks (can't insert element) when max.size is reached. We need to have a priority queue where the item with the lowest priority is discarded and replaced by the new item.\n",
    "<i>priority_queue</i> represents a min-heap.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to check wheter a certain element is already present in the datastructure (which is basically a list with tuples as elements) used to store the attributes and is used in both the priority-queue class and the implementation of the beam-search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alreadyInList(list1, list2):\n",
    "    permutations = list(itertools.permutations(list1))\n",
    "    for t in permutations:\n",
    "        if list(t) in list2:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "class priority_queue:\n",
    "    def __init__(self, max_size):\n",
    "        self.items = []\n",
    "        self.max = max_size\n",
    "   \n",
    "    def push(self, item, priority):\n",
    "        if ((len(self.items) < self.max) and (not alreadyInList(item, self.get_items()))):\n",
    "            heapq.heappush(self.items, (priority, item))\n",
    "        elif (not alreadyInList(item, self.get_items())):\n",
    "            heapq.heappushpop(self.items, (priority, item))\n",
    "    \n",
    "    def get_items(self):\n",
    "        result = []\n",
    "        for i in self.items:\n",
    "            result.append(i[1])\n",
    "        return result\n",
    "\n",
    "    def pop(self):\n",
    "        return heapq.heappop(self.items)\n",
    "\n",
    "    def get_max_item(self):\n",
    "        return self.items[0]\n",
    "    \n",
    "    def empty(self):\n",
    "        return not self.items\n",
    "    \n",
    "    def print_elements(self):\n",
    "        result = []\n",
    "        for i in self.items:\n",
    "            result.append(i)\n",
    "        return result\n",
    "       \n",
    "    def heap_sort(self):\n",
    "        return [heapq.heappop(self.items) for _ in range(len(self.items))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function <i>create_dataframe</i> is used to obtain a subset of the <i>df_action</i> dataframe, based on the the description language which is passed as an argument. The dataframe which is returned is the subgroup of records (users) belonging to that particular description language. For example, a group of users who all were using iOS as operating system at the time of the experiment. \n",
    "\n",
    "The <i>create_dataframe</i> function drops duplicate rows after each iteration. In other words, users that visited the particular version multiple times, with the same cookie-settings and the same action-type, are discarded from the dataset. It could be that a user is very fanatic in visiting the website and at every visit the user has the same action (view/click). This can influence the dataset, because it is not fair compared to users who visit the particular version less frequently. We don't remove all the duplicate user (id's), because sometimes a user clicked on the button in a particular version, while the other time the user did not click. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataframe(Set, df_org):\n",
    "    d =df_org.copy()\n",
    "    count = 0\n",
    "    for column, item in Set:\n",
    "        if count==0:\n",
    "            if isinstance(item, str): # single value\n",
    "                df_new = pd.merge(d, df_org.loc[(df_org[column] == item)], on=list(df_org), how='inner')\n",
    "            else: # list of values\n",
    "                df_new = pd.merge(d, df_org.loc[df_org[column].isin(item)], on=list(df_org), how='inner')\n",
    "            df_new.drop_duplicates(inplace = True)\n",
    "        else:\n",
    "            if isinstance(item, str): # single value\n",
    "                df_new = pd.merge(df_new, df_org.loc[(df_org[column] == item)], on=list(df_org), how='inner')\n",
    "            else: # list of values\n",
    "                df_new = pd.merge(df_new, df_org.loc[df_org[column].isin(item)], on=list(df_org), how='inner')\n",
    "        count +=1\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function <i>constraints</i> checks whether a certain subgroup, represented by a dataframe which is passed as an argument, satisfies the constraints $C$. At the moment the only constraint of a subgroup is that it should be represented by at least $2$% of the records (users) in the original dataset and at most $40$% of the dataframe. The upper bound is added because otherwise we get subgroups that are simply too large. Then you get subgroups that have a slight difference and therefore you can't draw any conclusions from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constraints(df_matches, df_org):\n",
    "    return (df_matches.shape[0] > (df_org.shape[0] *0.02)) & (df_matches.shape[0] < (df_org.shape[0] *0.40)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The refinement operator gets the records and chooses to which type of attribute they belong.\n",
    "There are 3 types of attributes:\n",
    "    1. Numeric: Attribute with only numbers as records\n",
    "    2. Binary: Attribute with boolean values (true or false) as records\n",
    "    3. Nominal: Attribute with multiple different values in their records which are not numeric\n",
    "    \n",
    "For the first level, we generate all patterns consisting of <i>one</i> condition on <i>one</i> attribute. All patterns are evaluated with the quality measure $\\varphi$ and the $w$ best are saved as the <i>beam</i>. The attributes `action`, `condition` and `user_id` are discarded from the list of attributes, because these are not relevant for defining a description language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_paterns_one_condition(y, df_org): # level 1\n",
    "    result = []\n",
    "    columns = list(df_org.columns.values)\n",
    "    not_relevant = ['action', 'condition', 'user_id']\n",
    "    columns = [x for x in columns if (x not in not_relevant)]\n",
    "    for column in columns:\n",
    "        if df_org[column].unique().size == 2: # binary attribute\n",
    "            result.append((column, True))\n",
    "            result.append((column, False))\n",
    "        elif is_numeric_dtype(df_org[column]): # numeric attribute\n",
    "        # equal height binning\n",
    "            n = df_org.shape[0]\n",
    "            values = sorted(df_org[column].values)\n",
    "            for a in range(1, y):\n",
    "                boundary_point = (a * n) / y\n",
    "                found_boundary_value = values[boundary_point] # value at boundary point\n",
    "                result.append(column, found_boundary_value)\n",
    "        else: # nominal attribute\n",
    "            for value in df_org[column].unique():\n",
    "                all_unique = list(df_org[column].unique())\n",
    "                complement_value = [x for x in all_unique if x != value]\n",
    "                result.append((column, value))\n",
    "                result.append((column, complement_value))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alluded to in the referenced paper, the StudyPortals (original) dataset comes naturally equiped with $m=2$ nominal targets. The first nominal target attribute is $condition$, which represents a binary column that tells us to which version the particular user was exposed during the experiment (i.e. version A or B). The second nominal target attribute is $action$, which is the binary column representing whether the page visitor merely viewed or also clicked on the button in question during the experiment. Considering these peculiarities of the StudyPortals dataset, the natural choice of EMM instance would be the association model class. So we strive to find subgroups for which the association between view/click and A/B is exceptional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      | View | Click |\n",
    "|------|------|-------|\n",
    "|   A  |$n_1$ | $n_2$ |\n",
    "|   B  |$n_3$ | $n_4$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what model class will be exploited, the next step is to define or exploit an appropriate quality measure. Since one can easily achieve huge deviations in target behaviour (assiociation between the differences in the two nominal target attributes), it makes sense to have a dimension in the quality measure which reflects the group size. In addition one also needs to have a target deviation dimension/component in the quality measure, of course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Target Deviation Component</b> ($\\varphi_{Q}(S)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first quality measure that is implemented, is Yule's Quality Measure as described in section $4.3$ of the A&B Testing paper. For the quality measure, we use the cells of the target contingency table, given in the table above. Given a subgroup $S\\subseteq \\Omega$, we can assign each record in $S$ to the appropiate cell of this contingency table, which leads to count values for each of the $n_i$ such that: $n_1 + n_2 + n_3 + n_4 = |S|$. Yule's Q is defined as: $\\frac{(n_1\\bullet n_4 - n_2\\bullet n_3)}{(n_1\\bullet n_4 + n_2 \\bullet n_3)}$. Higher numbers on the main diagonal implies a positive assocation between the two targets and higher numbers off the main diagonal implies a negative association between the two targets. The value for $Q$ instantiated by the subgroup $S$ is denoted by $Q_S$. We contrast Yule's Q instantiated by a subgroup $S$ with Yule's Q instantiated by that subgroup's complement $S^\\mathsf{C}$: $\\varphi_{Q}(S) = |Q_S - Q_{S^\\mathsf{C}} |$. This component detects subgroups whose view/click-A/B association is different from the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_deviation(df_matches):\n",
    "    zero = np.finfo(np.double).tiny\n",
    "    \n",
    "    n_1 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_2 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_3 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    n_4 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    Q_s = (n_1*n_4 - n_2*n_3)/(n_1*n_4 + n_2*n_3)\n",
    "    \n",
    "    df_complement = df_matches.merge(df_action, indicator=True, how='outer')\n",
    "    df_complement = df_complement[df_complement['_merge'] == 'right_only']\n",
    "    df_complement.drop(['_merge'], axis=1, inplace = True)\n",
    "    \n",
    "    n_c_1 = df_complement.loc[(df_complement.action == 'view') & (df_complement.condition == '1-Control')].count()[0]\n",
    "    n_c_2 = df_complement.loc[(df_complement.action == 'clic') & (df_complement.condition == '1-Control')].count()[0]\n",
    "    n_c_3 = df_complement.loc[(df_complement.action == 'view') & (df_complement.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    n_c_4 = df_complement.loc[(df_complement.action == 'clic') & (df_complement.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "\n",
    "    Q_s_c = (n_c_1*n_c_4 - n_c_2*n_c_3)/(n_c_1*n_c_4 + n_c_2*n_c_3)\n",
    "    \n",
    "    if (math.isnan(Q_s_c)):\n",
    "        Q_s_c = zero\n",
    "    if (math.isnan(Q_s)):\n",
    "        Q_s = zero\n",
    "\n",
    "    phi_Q_S = abs(Q_s - Q_s_c)\n",
    "    \n",
    "    return phi_Q_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Subgroup Size Component</b><br>\n",
    "To represent the subgroup size, we take the entropy function as described in section $3.1$ of the referenced paper. The function conceptually rewards $50/50$ splits between subgroup and complement, while punishing subgroups that are either (relatively) small or cover the vast majority of the dataset.\n",
    "\n",
    "$\\varphi_{ef}(D) = - \\frac{n}{N}lg(\\frac{n}{N}) - \\frac{n^C}{N}lg(\\frac{n^C}{N})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_function(df_matches):\n",
    "    zero = np.finfo(np.double).tiny # dealing with divisions/logarithms of 0\n",
    "    n = df_matches.shape[0] # size subgroup in original dataset\n",
    "    N = df_action.shape[0] # size original dataset\n",
    "    n_c = N - n # complement of subgroup in orginal dataset\n",
    "    if (n == 0):\n",
    "        n = zero\n",
    "    if (n_c == 0): \n",
    "        n_c = zero\n",
    "    return ((-(n/N) * math.log2(n/N)) - ((n_c/N) * math.log2(n_c/N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When combining these two components/dimensions, one obtains an association model class quality measure known as <i>Yule's Quality Measure</i>. $\\varphi_{Yule}(S) = \\varphi_{Q}(S) \\cdot \\varphi_{ef}(S) $, which boils down to a multiplication of the target deviation and the subgroup size components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phiYule(df_matches):\n",
    "    return target_deviation(df_matches) * entropy_function(df_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiplication of the two components ensures that subgroups are evaluated well (i.e. score well) on both components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is responsible for the generation of new candidate patterns for the $n^{th}+1\\,level$ by refining patterns from the $n^{th}\\,level$ beam (denoted by <i>current_beam</i>). A pattern is refined into many new candidates by the conjuction of each possible single condition on a single attribute. The function results a list with all the candidates. The $beam-search$ algorithm should evaluate all $level\\,n+1$ candidates with $\\varphi$ and store the $w$ best one as the new beam. In addition, it should update the list of $q-$best-performing subgroups, if there are new candidates which surpass the current top$-q$ in terms of the particular quality measure $\\varphi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refine(current_beam, d):\n",
    "    w = len(current_beam)\n",
    "    result= []\n",
    "    for i in range(0, w):\n",
    "        for j in range(0, w):\n",
    "            if (i != j):\n",
    "                len_i = len(current_beam[i])\n",
    "                len_j = len(current_beam[j])\n",
    "                min_len = min(len_i, len_j)\n",
    "                new = []\n",
    "                for t in (0, min_len - 1):\n",
    "                    if(current_beam[i][t] not in new):\n",
    "                        new.append(current_beam[i][t])\n",
    "                    if(current_beam[j][t] not in new):\n",
    "                        new.append(current_beam[j][t])\n",
    "                if ((not alreadyInList(new, current_beam)) & (len(new) == d)):\n",
    "                    result.append(new)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the implementation of the $beam\\_search$ algorithm. There are a few parameters which influence the outcome, and can be changed as one sees fit for a certain experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 2 # search depth\n",
    "w = 10 # search width (i.e. beam width)\n",
    "q = 5 # size of the set of best subgroup results (i.e. top q subgroups)\n",
    "y = 10 # number of in which numeric descriptors are dynamically discretized\n",
    "quality_measure = phiYule # choose between phiYule (Yule's Quality Measure), maxDeviationMeasure (max deviation measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(d, w, q, quality_measure, df_org):\n",
    "    resultSet = priority_queue(q)\n",
    "    beam = priority_queue(w)\n",
    "    # first level\n",
    "    all_paterns = all_paterns_one_condition(y, df_org)\n",
    "    print(\"level: \"+str(1))\n",
    "    for desc in all_paterns:\n",
    "        subset_desc = create_dataframe([desc], df_org)\n",
    "        if (quality_measure == phiYule): # two versions\n",
    "            quality = quality_measure(subset_desc)\n",
    "        else: # more than two versions\n",
    "            quality = quality_measure(subset_desc)[1]\n",
    "        if (constraints(subset_desc, df_org)):\n",
    "            beam.push([desc], quality)\n",
    "            resultSet.push([desc], quality)\n",
    "    for level in range(2, d + 1):\n",
    "        print(\"level: \"+str(level))\n",
    "        current_beam = beam.get_items()\n",
    "        new_candidates = refine(current_beam, level)\n",
    "        for c in new_candidates:\n",
    "            subset_desc = create_dataframe(c, df_org)\n",
    "            if (quality_measure == phiYule): # two versions\n",
    "                quality = quality_measure(subset_desc)\n",
    "            else: # more than two versions\n",
    "                quality = quality_measure(subset_desc)[1]\n",
    "            if (constraints(subset_desc, df_org)):\n",
    "                beam.push(c, quality)\n",
    "                resultSet.push(c, quality)\n",
    "    return resultSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 1\n",
      "level: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.34039077670350143, [('geo_country', 'DE')]),\n",
       " (0.3647984213664936, [('browser_language', 'latin_lan'), ('os_name', 'iOS')]),\n",
       " (0.47027223676731572,\n",
       "  [('browser_language',\n",
       "    ['greek', 'latin_lan', 'asian', 'cyrillic', 'herbrew']),\n",
       "   ('os_name', 'iOS')]),\n",
       " (0.50998200142197703, [('geo_country', 'DE'), ('os_name', 'iOS')]),\n",
       " (0.56310255294633826, [('os_name', 'iOS')])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = beam_search(d = d, w = w, q = q, quality_measure = quality_measure, df_org = df_action)\n",
    "sorted_result = priority_queue.heap_sort(result)\n",
    "sorted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation given above allows the end user to:\n",
    "* manually set the beam width $w$ and search depth $d$\n",
    "* manually choose the number of bins $y$ in which numeric desciptors are dynamically discretized\n",
    "* easily swap out the association model class on these specific two targets for another model class (to be coded by the end user) with any number of targets of the user's choosing (by changing the $quality\\_measure$ (which is related to a particular model class) parameter to the model class one wants to use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code used to fill in the table (see 2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yule_Q_multiple_return(df_matches):\n",
    "    zero = np.finfo(np.double).tiny\n",
    "    \n",
    "    n_1 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_2 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_3 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    n_4 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    Q_s = (n_1*n_4 - n_2*n_3)/(n_1*n_4 + n_2*n_3)\n",
    "    \n",
    "    df_complement = df_matches.merge(df_action, indicator=True, how='outer')\n",
    "    df_complement = df_complement[df_complement['_merge'] == 'right_only']\n",
    "    df_complement.drop(['_merge'], axis=1, inplace = True)\n",
    "    \n",
    "    n_c_1 = df_complement.loc[(df_complement.action == 'view') & (df_complement.condition == '1-Control')].count()[0]\n",
    "    n_c_2 = df_complement.loc[(df_complement.action == 'clic') & (df_complement.condition == '1-Control')].count()[0]\n",
    "    n_c_3 = df_complement.loc[(df_complement.action == 'view') & (df_complement.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    n_c_4 = df_complement.loc[(df_complement.action == 'clic') & (df_complement.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "\n",
    "    Q_s_c = (n_c_1*n_c_4 - n_c_2*n_c_3)/(n_c_1*n_c_4 + n_c_2*n_c_3)\n",
    "    \n",
    "    if (math.isnan(Q_s_c)):\n",
    "        Q_s_c = zero\n",
    "    if (math.isnan(Q_s)):\n",
    "        Q_s = zero\n",
    "\n",
    "    phi_Q_S = abs(Q_s - Q_s_c)\n",
    "    \n",
    "    return {\"phi_Q_S\": phi_Q_S,\"Q_s\": Q_s, \"Q_s_c\": Q_s_c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_1 = sorted_result[0][1]\n",
    "df_S1 = create_dataframe(S_1, df_action)\n",
    "size_S1 = df_S1.shape[0]\n",
    "size_S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q_s': -0.12408759124087591,\n",
       " 'Q_s_c': 0.38916994566235713,\n",
       " 'phi_Q_S': 0.51325753690323306}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yule_S1 = yule_Q_multiple_return(df_S1)\n",
    "yule_S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_2 = sorted_result[1][1]\n",
    "df_S2 = create_dataframe(S_2, df_action)\n",
    "size_S2 = df_S2.shape[0]\n",
    "size_S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q_s': -0.69465648854961837,\n",
       " 'Q_s_c': 0.32473061061593722,\n",
       " 'phi_Q_S': 1.0193870991655556}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yule_S2 = yule_Q_multiple_return(df_S2)\n",
    "yule_S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_3 = sorted_result[2][1]\n",
    "df_S3 = create_dataframe(S_3, df_action)\n",
    "size_S3 = df_S3.shape[0]\n",
    "size_S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q_s': -0.6216216216216216,\n",
       " 'Q_s_c': 0.3514468430908384,\n",
       " 'phi_Q_S': 0.97306846471246}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yule_S3 = yule_Q_multiple_return(df_S3)\n",
    "yule_S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_4 = sorted_result[3][1]\n",
    "df_S4 = create_dataframe(S_4, df_action)\n",
    "size_S4 = df_S4.shape[0]\n",
    "size_S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q_s': -0.6908212560386473,\n",
       " 'Q_s_c': 0.38767876787678768,\n",
       " 'phi_Q_S': 1.078500023915435}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yule_S4 = yule_Q_multiple_return(df_S4)\n",
    "yule_S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_5 = sorted_result[4][1]\n",
    "df_S5 = create_dataframe(S_5, df_action)\n",
    "size_S5 = df_S5.shape[0]\n",
    "size_S5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q_s': -0.08771929824561403,\n",
       " 'Q_s_c': 0.49407114624505927,\n",
       " 'phi_Q_S': 0.5817904444906733}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yule_S5 = yule_Q_multiple_return(df_S5)\n",
    "yule_S5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b - Found Subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <i>beam search</i> algorithm is executed with parameters $d=2, w = 5$ and $q = 5$. We choose for $d=2$, a conjunction of at most $2$ conditions on single descriptors, because of interpretability. When $d>2$, the results become more complex and therefore give less information on which a domain expert can act. With $q = 5$, the output of <i> beam search</i> can be easily compared to the top five subgroups found in the A&B Testing paper. The biggest influence on the results was the maximum size of the subgroup that is allowed. This constraint is added in the <i>constraints</i> function. When adding no upper bound, the top $q$ results contained subgroups representing over $85$% of the dataset, with very small differences between the values of them. We choose $w=10$, so that the search width of the beam is twice the top $q$ best subgroups. Larger values for $w$ were tested, but they did not change the result a lot. As one can see the <i>beam search</i> is executed with the same parameters as in the referenced A/B Testing paper. This also makes it much easier to compare the results, however one must note that the dataset `df_action`, preprocessed in question 1, contains $899$ records while the dataset that is used in the referenced A/B Testing paper contains over $3500$ records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26575602240066371"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_deviation(df_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset $\\Omega$ that is preprocessed in question 1 has a total number of 899 records. Yule's Q has a value of $\\varphi_Q(\\Omega) = 0.27$. In other words, the result of the traditional A/B test tells us that variant B: the more buttony variantion generates more clicks than the less buttony control version. The new variation is therefore slightly better, however it can be argued whether the difference is significant. In A&B Testing, we can mine deeper into the data and find specific subgroups which prefer version A or version B. This allows us to present each found subgroup whith its own preferable version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-five subgroups found are presented in the table below, in order of descending quality. Let $S_1$ denote the best subgroup and let $S_5$ denote the subgroup with the lowest $\\varphi_{Yule}(S)$ value. Besides the values of the quality measure $\\varphi_{Yule}$, the value of the Yule's Q component on both the subgroup and its complement, as well as the subgroup size component are presented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "    <th>Subgroup definition</th>\n",
    "        <th>$\\varphi_{Yule}(S)$</th>\n",
    "        <th>$\\varphi_{Q}(S)$</th>\n",
    "        <th>$\\varphi_{Q}(S^C)$</th>\n",
    "        <th>$|S|$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>os_name = \"iOS\"</td>\n",
    "        <td>$0.56$</td>\n",
    "        <td>$-0.09$</td>\n",
    "        <td>$0.49$</td>\n",
    "        <td>$355$</td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td>geo_country = \"DE\" $\\wedge$ os_name = \"iOS\"</td>\n",
    "        <td>$0.51$</td>\n",
    "        <td>$-0.69$</td>\n",
    "        <td>$0.38$</td>\n",
    "        <td>$91$</td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td>browser language = \"greek\",\"latin_lan\",\"asian\", \"cyrillic\", \"herbrew\" $\\wedge$ os_name = \"iOS\"</td>\n",
    "        <td>$0.47$</td>\n",
    "        <td>$-0.62$</td>\n",
    "        <td>$0.35$</td>\n",
    "        <td>$94$</td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td>browser_language = \"latin-lan\" $\\wedge$ os_name = \"iOS\"</td>\n",
    "        <td>$0.36$</td>\n",
    "        <td>$-0.69$</td>\n",
    "        <td>$0.32$</td>\n",
    "        <td>$61$</td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td>geo_country = \"DE\"</td>\n",
    "        <td>$0.34$</td>\n",
    "        <td>$-0.12$</td>\n",
    "        <td>$0.39$</td>\n",
    "        <td>$155$</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we explain the top-five subgroups found, we first clarify what a positive/negative value for Yule's Q means. First, note that version A denotes the control (current) version of the website, while version B denotes the new version with a more buttony styled button. A positive value for Yule's Q value implies a positive association between the two targets. In other words, a positive value for Yule's Q tells us that people presented with web page variant B (the more buttony button variant) click the button more often than people presented with web page variant A. The same holds for when Q is negative, then the subgroup presented with variant A clicks the button more often than people presented with variant B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best subgroup found, denoted as $S_1$, is defined by people who visit the web page from an iOS-device. This subgroup is also the largest in size among the other subgroups and represents almost $40$% of the total dataset. One can see that the Q-value on $S_1$ is slightly negative. It is therefore hard to say anything about whether iOS users prefer the old version A, but it is very clear that they don't click more often when they are presented with the more buttony version. The buttony button is not in the characteristic flat iOS design style and this may have something to do with whether the iOS user clicks the more modern button version more often than the buttony button version, presented in website B. We also see that Yule's Q value on the complement of $S_1$ has a positive value. So Android, Windows, and 'other' users click the buttony button version more often than the modern version. For Windows users, a buttony button is more familiar, because these kind of buttons are widely used in desktop applications. There are also lots of Android phones that run an old version of the Android OS. These versions don't have the new flat design, and therefore for these users the new variant B might be more familiar and more in line with the style of their operating system. Because subgroup $S_1$ has a substantial size, this is a subgroup on which the domain expert can act. Presenting iOS users the more modern version of the button and Android, Windows, and 'other' users the more buttony version increases the overal revenue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second, third and fourth subgroups are specializations of $S_1$. One can see that iOS users combined with a second condition in the description language gives a significantly lower value (or: higher negative value) for Yule's Q. Let's start with the second best subgroup, $S_2$. $S_2$ represents iOS users from Germany. Compared to Yule's Q value of $S_1$, there is a significant difference. Apparantly, iOS users from Germany click less the button on version B less often compared to version A. As we can read from the Yule's Q value on the complement subgroup, we see that iOS users that are not from Germany click the button on version B more often compared to users that are not from Germany that get presented with version A.\n",
    "\n",
    "The third subgroup requires more attention. This description language contains many values for `browser_language`. It is therefore easier to first take a look at the complement of this subgroup. The complement of this dataframe are the users who have either not set up the browser language to one of these values <i>or</i> the users who have visited the website from an Android, Windows or 'other' device.  To identify the complement of these browser languages, we execute the following code. The following cell creates a dataframe with only iOS users and where the browser_language does not equal one of the browser language values that is in $S_3$. Finally it outputs the unique values in the column `browser_language`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function returns the complement dataframe of the subgroup, given as a parameter. This function is part of the original <i>target_deviation</i> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_complement(df_matches, df_org):\n",
    "    df_complement = df_matches.merge(df_org, indicator=True, how='outer')\n",
    "    df_complement = df_complement[df_complement['_merge'] == 'right_only']\n",
    "    df_complement.drop(['_merge'], axis=1, inplace = True)\n",
    "    return df_complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['english'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_3 = sorted_result[2][1]\n",
    "df_S3 = create_dataframe(S_3, df_action)\n",
    "df_S3_c = create_complement(df_S3, df_action)\n",
    "df_S3_c = df_S3_c.loc[(df_S3_c.os_name == 'iOS')]\n",
    "df_S3_c['browser_language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only browser language that is in this dataframe is English. Yule's Q on $S^C$ is positive. In other words, users that are either non-iOS users <i>or</i> users that have set their browser language to English, click the more buttony button more often, compared with the same subgroup that is shown the more modern version of the button. Just as for $S_2$, $S_3$ is a subgroup of users who click on the button in version A more often, compared to the same group of users who visit version B.\n",
    "Recall that the dataset contains 899 records. This means that $S_2$ and $S_3$ represent $10$% of the dataset, which is a substantial percentage. Therefore, it is useful for the domain expert to give German iOS users the more modern version, as well as the users who have set their browser language to English. \n",
    "The fourth ranked subgroup $S_4$ represents iOS users who have set their browser_language to Latin. This is the smallest subgroup in size among all the other subgroups, so we have to take care about what we are going to see about this subgroup. However, it's clear that people who have set their browser language to Latin and are iOS users, definetely do not click version B more often compared to the same group of people who visited the webpage with the more modern styled button version A. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth ranked subgroup $S_5$ represents, just as $S_1$, a substantial part of the total dataset. This description language is very clear: German visitors don't click more on the new buttony button version, compared to German users who are shown the control version of the buttons. There is quite a difference between the subgroup and its complement. The users in the complement of this subgroup, $S^C_5$, click on the modern button more often when visiting version A, compared to non-German users who visit the website with the buttony buttons. Because of the size of $S_5$, this is a potential subgroup on which StudyPortal can focus. Visitors that are not from Germanny click on the buttony buttons more often, and therefore using these buttons increases the overall revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between found subgroups\n",
    "When we compare the discovered subgroups to the subgroups found in the referenced A/B Testing paper, there are a few noteable differences. First, we see that iOS dominates the subgroups that we found above, with four of the five subgroups containting iOS as `os_name`. The referenced paper only contains one subgroup with iOS users. Also, the conclusion and results about this subgroup are very different compared to what we concluded above. The referenced paper concludes that visitors who run the iOS operating system strongly prefer version B. In the paper this is noted as remarkable, because the buttons of version B do not conform to Apple's design standards. In the results of the discovered subgroups we concluded the opposite. iOS users strongly prefer version A. This seems to make more sense, because this button is more in line with the modern, flat look of the iOS operating system. Because iOS users are dominated in the top-five subgroups of our result, we can say with confidence that iOS users prefer version A above B. The ones who have not set their browser language to English or are non-iOS users click the buttony button more often, which is also the conclusion in the A/B testing paper. The subgroups that are identified in the <i>beam search</i> algorithm we implemented, $S_2$, $S_4$ and $S_5$ are not identified in the top five subgroups of the A/B testing paper. This could be a difference in the <i>beam search</i> implementation, but also note that the dataset that is used in the paper is almost four times larger than the one we have used. Of course this has a big influence on the output. The subgroups that are different from the subgroups that are found in the paper are all explained above and don't need any further explanation in this paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The top five subgroups that are found with <i>beam search</i> can be found in the table above. The largest schism lies between people who visit the website from an iOS device, who strongly prefer version A, and people who visit the website from an Android, Windows or 'other' device. The latter group strongly prefers version B, the more buttony button version. Because of the sizes of these subgroups, the overall revenue will substantially increase when iOS users are shown version A and Android, Windows, and 'other' users are shown version B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"red\"><b>This is assignment is made in Jupyter notebooks, if you (the end user/reviewer/evaluator) want to change the parameters (e.g. the beam width $w$ and/or the search depth $d$) of our implementation of the beam-search algorithm, you can download the notebook file by clicking the following link: <a href=\"https://www.dropbox.com/sh/y81uxp5oahfzy76/AADHMc0Ofm5QWnPMrzX7dTKLa?dl=0\">Beam-Search implementation Notebook and pre-processed data</a>. This particular notebook can be found in the file \"Beam_search-V2-NewModelClass.ipynb\". The link will remain live unitl 01 February 2018.</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Beyond A&B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Question 2 we have explored the straightforward solution to StudyPortals’ inquiry, however, there is more to explore. In Question 3, we will design an EMM instance that extracts more information out of the StudyPortals situation. So we make a shift from implementing existing ideas, to contributing new ones to this field of research and possibly inspired by already existing ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. The design of a new EMM instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B/C/D Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what would have happened if StudyPortals had designed four different variations of their webpage. For example, they might not only be interested in the two different button designs that were mentioned, but they might have multiple designs such as these, let's say for four different types of buttons. As such, we would not be performing a simple A/B test, but instead we would be doing an A/B/C/D test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regular A/B testing, we had targets $t_1$, the binary column representing whether the page visitor merely viewed or also clicked, and $t_2$, the binary column representing whether the visitor was presented version A or B of the buttons. Therefore, the natural choice of EMM instance was the association model class, which allows us to determine the association between two nominal targets.\n",
    "\n",
    "Now, with A/B/C/D testing, it still makes sense to use the association model, because we are still using two nominal targets. In this case, our targets will be $x$, the binary column representing whether the page visitor merely viewed or also clicked, and $y$, the numerical column representing whether the visitor was presented version A, B, C, or D of the webpage.\n",
    "\n",
    "Even though we will be using integers (1, 2, 3, 4 for version A, B, C, D respectively) to code the distinct values of $y$, their values will be treated as unordered (nominal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fixed the model class, we need to define an appropriate quality measure. To ensure the discovery of subgroups that represent substantial effects within the datasets, a common approach is to craft a quality measure by multiplying two components: one representing the target deviation, and one representing the subgroup size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      | View | Click |\n",
    "|------|------|-------|\n",
    "|   A  |$V_A$ | $C_A$ |\n",
    "|   B  |$V_B$ | $C_B$ |\n",
    "|   C  |$V_C$ | $C_C$ |\n",
    "|   D  |$V_D$ | $C_D$ |\n",
    "\n",
    "<b><i>Table 1: Target-cross table</i></b><br>\n",
    "Where $V_X$, $C_X$ denotes the number of views and the number of clicks, respectively, found for class $X$ for a certain group within the dataset (described by a certain (group) of attribute values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target deviation component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the quality measure component representing the target deviation, we use the target contingency table, shown in Table 1. Now, since this is no longer a 2x2 matrix, we cannot use Yule's Quality Measure here. Instead, we will have to define another appropriate quality measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting (and logically sound) target deviation statistic for a subgroup would be the maximum deviation found across the classes. One can imagine that we would like to know what the highest deviation is that any of the subgroups have from the original dataset across all the models present in the dataset. Let us specifiy this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we compute the average for each of the classes in the orginal dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ClickTotal_X$ denote the number of records in the orginal datastet of class $X$ (i.e. version \"$X$\") in which a click was registered. Similarly, let $ViewTotal_X$ denote the number of records in the original dataset of class $X$ for which a view was registered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$avgTotal_A = ClickTotal_{A} \\,/\\, (ViewTotal_{A} + ClickTotal_{A})$<br>\n",
    "$avgTotal_B = ClickTotal_{B} \\,/\\, (ViewTotal_{B} + ClickTotal_{B})$<br>\n",
    "$avgTotal_C = ClickTotal_{C} \\,/\\, (ViewTotal_{C} + ClickTotal_{C})$<br>\n",
    "$avgTotal_D = ClickTotal_{D} \\,/\\, (ViewTotal_{D} + ClickTotal_{D})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a particular subgroup we then calculate<br>\n",
    "$a_{group} = C_A \\,/\\, (V_A + C_A)$ <br>\n",
    "$b_{group} = C_B \\,/\\, (V_B + C_B)$ <br>\n",
    "$c_{group} = C_C \\,/\\, (V_C + C_C)$ <br>\n",
    "$d_{group} = C_D \\,/\\, (V_D + C_D)$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the deviation for each version now becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$devVersion_A = |\\,a_{group} - avgTotal_A\\,|$<br>\n",
    "$devVersion_B = |\\,b_{group} - avgTotal_B\\,|$<br>\n",
    "$devVersion_C = |\\,c_{group} - avgTotal_C\\,|$<br>\n",
    "$devVersion_D = |\\,d_{group} - avgTotal_D\\,|$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which will result in a value between 0 and 1, encapsulating the deviation for a particular group (subset of the original dataset, filtered on certain attribute value(s)) for a particular class (i.e. \"version\" in our problem context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final target deviation component for this particular group becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$max\\_devGroup = \\max \\{devVersion_A, \\,devVersion_B, \\,devVersion_C, \\,devVersion_D\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a perfect target deviation component considering the (unbounded) number of classes one should be able to expose to this EMM instance. Imagine having a certain group which would show large deviations for a particular class, just taking the average deviations would result in skewed results since it also takes into account the deviations for other classes. This would affect the relative ranking of the subgroups in the next step of this elaboration. Using this particular target deviation component will allow one to compare groups which show a certain deviation for a certain class, instead of constantly comparing the averages or another statistical estimator. You want to rate the deviations for subgroups, but considering the unbounded nature of the classes in this EMM instance, it makes sense to rate the subgroups for their deviations for <b>one</b> class (the maximum). It could be that you get a list of subgroups rated by different classes, but this does not matter for the results. If you want a list of the top-5 subgroups found in the dataset (with certain constraints), it does not matter for which of the classes the subgroups showed a deviation from the original dataset. You eventually want to know for which class a top-group is showing the deviation, but this is an implementation detail which we will tackle later on. In Yule Q's one could simply look at whether a value is positive/negative to know to which class it belongs, which is no longer possible if one would take a statistical measure over the deviation for each class (e.g. average), which is why we choose this approach (the maximum).\\\\\n",
    "\n",
    "This also tackles the subgroup-size component of the quality measure, one can simply pass the model class which had the highest deviation to the function responsible for the subgroup-size component of the quality measure. Which immediately brings us to the subgroup size component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgroup-size component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target deviation component is now defined as $max\\_devGroup = \\max \\{devVersion_A, \\,devVersion_B, \\,devVersion_C, \\,devVersion_D\\}$. If the maximum deviation is found for class X, we can simply use the same entropy function as was used in question 2. A brief recap:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent the subgroup size, we take the entropy function as described in section $3.1$ of the referenced paper. The function conceptually rewards $50/50$ splits between subgroup and complements, while punishing subgroups that are either (relatively) small or cover the vast majority of the dataset.\n",
    "\n",
    "$\\varphi_{ef}(D) = - \\frac{n}{N}lg(\\frac{n}{N}) - \\frac{n^C}{N}lg(\\frac{n^C}{N})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case one would filter the group (already a subset of the original dataset of $N$ records) on the modelclass ($condition$) $X$. The size (number of records) of this subgroup for the model class which showed the maximum deviation, is in this case  the $n$ in the entropy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the target-size and subgroup-size component: Max-dev measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to combine the two compontents into one final quality measure which can be used to rate the subgroups based on the deviations they show for a certian model class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\varphi_{maxdev}(D) = \\varphi_{ef}(D) * max\\_devGroup(D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We multiply the two components to have a quality measure which captures the maximum target deviation found across the classes (i.e. versions), while having a dimension/component in the quality measure which represents the subgroup size, considering the fact that one can easily achieve huge deviations in target behaviour in (very) small subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revision of the given answers (3a) and the proposed ideas which be implemented "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this quality measure focuses on the deviation found for one particular class. The quality measure does not focus on/capture how the subgroup deviates from the norm across all classes. It checks each class seperately for the subgroup and reports the maximum deviation found, which makes sense in the problem context of \"A&B Testing\", since you want to be able to present a certain group one version (i.e. class) of the website, for which the subgroup showed deviation from the norm.\n",
    "\n",
    "<i>The choice is made to design an EMM instance for the situation in which subgroups can be researched when there are multiple (i.e. more than 2) versions of a website/application available. In particular, the situation in which 4 versions of the StudyPortals website is picked as an example, and will be examined in more detail in the next section (3b). This examination of the model class and related quality measure will be done by artificially modifying the orginal dataset, since these versions are, of course, not present in the original dataset.</i>\n",
    "\n",
    "This EMM instance can be used to test different versions of a website/application, but it is not suitable (just like the EMM instance in the referenced paper) for the evaluation of deviations among subgroups of components which are related to each other. So there needs to be one particular component which is different amongst the versions (i.e. the styling of a button). This model class is not suitable to test the performance (i.e. target behaviour) of different types of components of a webpage (i.e. font, button styling, colours, reponse times, contrast etc.). Another model class and related quality measure would need to be designed for such an experiment.\n",
    "\n",
    "So the EMM instance is able to extract additional information when there are more than 2 versions available. The quality measure assesses for a particular group (described by a (group of) attribute values; description languange) and each different version used in the experiment, the target deviation relative to the target statistics of that particular version in the orignal (complete) dataset. It then picks the class (version) which has the highest target deviation and evaluates the subgroup-size component of this subset. The target deviation component and subgroup-size component are then combined into the final quality measure. This quality meausure can then be implemented in algorithms such as beam-search. The implementation has to make sure that the end-user is able to extract the actual class (version), for which a subgroup showed interesting deviation for the target component, corrected by the subgroup size component. This quality measure is then useful for experiments in which there are $>2$ versions of a particular website/application, and one wants to find subgroups which show unusual target behaviour for <b>one</b> of these versions. This information can, for example, be used to have dynamic webpages and/or components, based on the subgroup to which a visitor belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Implementation of the model class & related quality-measure into the beam-search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can actually start with the implementation of the new quality measure to find interesting subgroups with the new EMM instance, we have to modify the existing dataset. In the original dataset there were two conditions (versions) present. A control version (<i>1-Control</i>) and the experimental version (<i>2-Buttony-Conversion-Buttons</i>). The EMM instance which has been designed in section $3a$ is particularly interesting when there are $>2$ versions evaluated in an experiment. Unfortunately, there is no data available of such an experiment (done by StudyPortals), which is why the choice is made (as proposed in the assignment) to alter the original dataset, for the sake of research! (i.e. the experimental evaluation of the newly designed EMM instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>user_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>refr_source</th>\n",
       "      <th>browser_language</th>\n",
       "      <th>os_name</th>\n",
       "      <th>os_timezone</th>\n",
       "      <th>dvce_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clic</td>\n",
       "      <td>379881d5-32d7-49f4-bf5b-81fefbc5fcce</td>\n",
       "      <td>1-Control</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>greek</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clic</td>\n",
       "      <td>2a0f4218-4f62-479b-845c-109b2720e6e7</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clic</td>\n",
       "      <td>a511b6dc-2dca-455b-b5e2-bf2d224a5505</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>GB</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clic</td>\n",
       "      <td>9fb616a7-4e13-4307-ac92-0b075d7d376a</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clic</td>\n",
       "      <td>64816772-688d-4460-a591-79aa49bba0d5</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>BD</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  action                               user_id                     condition  \\\n",
       "0   clic  379881d5-32d7-49f4-bf5b-81fefbc5fcce                     1-Control   \n",
       "1   clic  2a0f4218-4f62-479b-845c-109b2720e6e7  2-Buttony-Conversion-Buttons   \n",
       "2   clic  a511b6dc-2dca-455b-b5e2-bf2d224a5505  2-Buttony-Conversion-Buttons   \n",
       "3   clic  9fb616a7-4e13-4307-ac92-0b075d7d376a  2-Buttony-Conversion-Buttons   \n",
       "4   clic  64816772-688d-4460-a591-79aa49bba0d5  2-Buttony-Conversion-Buttons   \n",
       "\n",
       "  geo_country refr_source browser_language  os_name os_timezone dvce_type  \n",
       "0          FI      Google            greek  Android      Europe    Mobile  \n",
       "1          AU      Google          english      iOS   Australia    Mobile  \n",
       "2          GB      Google          english  Android      Europe    Mobile  \n",
       "3          FI      Google          english      iOS      Europe    Mobile  \n",
       "4          BD      Google          english  Android        Asia    Mobile  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_action_copy = df_action.copy() # copy the original dataset\n",
    "df_action_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment we have two conditions (i.e. versions) <i>\"1-Control\"</i> and <i>\"2-Buttony-Conversion-Buttons\"</i>. Since we want to evaluate and implement the new EMM instance, we are going to introduce 2 more conditions (versions): <i>\"3-Button\"</i> and <i>\"4-Button\"</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice is made not to add new artificial rows with random values, but to copy all the rows and overwrite values in the $condition$ column of these copied rows with the new versions. The 899 records present in the original dataset will thus remain intact. These 899 records are going to be extended with 899 new records, but with random values for the $condition$ attribute (i.e. <i>\"3-Button\"</i> or <i>\"4-Button\"</i>). The result is a dataset of $2 * 899 = 1798$ records, half of which is real data and half of which is artificial data. This method is chosen since it seems the most scientifically sound method. Instead of generating values which artificially describe users, the dataset keeps attribute combinations which describe the real users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_four_conditions = pd.concat([df_action_copy, df_action_copy])\n",
    "df_four_conditions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new, artificial rows need to have new unique id's, since we require that each user is only exposed to one version of the website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(df_action_copy.shape[0], df_four_conditions.shape[0]):\n",
    "    df_four_conditions.user_id.iloc[i] = hex(random.getrandbits(128))[2:]  # random user_id's for the new artificial, users\n",
    "    # [2:] is used to remove the \"0x\" prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new rows need to be assigned new values for the $condition$ column. This is going to be done at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(df_action_copy.shape[0], df_four_conditions.shape[0]):\n",
    "    randomBit = random.randint(0, 1)\n",
    "    if(randomBit):\n",
    "        df_four_conditions.condition.iloc[i] = \"3-Button\"\n",
    "    else:\n",
    "        df_four_conditions.condition.iloc[i] = \"4-Button\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a csv copy of this dataframe to ensure stability of the randomness in the further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_four_conditions.to_csv('./data/df_four_conditions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>user_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>refr_source</th>\n",
       "      <th>browser_language</th>\n",
       "      <th>os_name</th>\n",
       "      <th>os_timezone</th>\n",
       "      <th>dvce_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clic</td>\n",
       "      <td>379881d5-32d7-49f4-bf5b-81fefbc5fcce</td>\n",
       "      <td>1-Control</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>greek</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clic</td>\n",
       "      <td>2a0f4218-4f62-479b-845c-109b2720e6e7</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clic</td>\n",
       "      <td>a511b6dc-2dca-455b-b5e2-bf2d224a5505</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>GB</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clic</td>\n",
       "      <td>9fb616a7-4e13-4307-ac92-0b075d7d376a</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clic</td>\n",
       "      <td>64816772-688d-4460-a591-79aa49bba0d5</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>BD</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  action                               user_id                     condition  \\\n",
       "0   clic  379881d5-32d7-49f4-bf5b-81fefbc5fcce                     1-Control   \n",
       "1   clic  2a0f4218-4f62-479b-845c-109b2720e6e7  2-Buttony-Conversion-Buttons   \n",
       "2   clic  a511b6dc-2dca-455b-b5e2-bf2d224a5505  2-Buttony-Conversion-Buttons   \n",
       "3   clic  9fb616a7-4e13-4307-ac92-0b075d7d376a  2-Buttony-Conversion-Buttons   \n",
       "4   clic  64816772-688d-4460-a591-79aa49bba0d5  2-Buttony-Conversion-Buttons   \n",
       "\n",
       "  geo_country refr_source browser_language  os_name os_timezone dvce_type  \n",
       "0          FI      Google            greek  Android      Europe    Mobile  \n",
       "1          AU      Google          english      iOS   Australia    Mobile  \n",
       "2          GB      Google          english  Android      Europe    Mobile  \n",
       "3          FI      Google          english      iOS      Europe    Mobile  \n",
       "4          BD      Google          english  Android        Asia    Mobile  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_four_conditions = pd.read_csv('data/df_four_conditions.csv')\n",
    "df_four_conditions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the newly designed quality measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The target deviation component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clickTotalA = df_four_conditions.loc[(df_four_conditions.condition == \"1-Control\") & (df_four_conditions.action == \"clic\")].shape[0]\n",
    "clickTotalB = df_four_conditions.loc[(df_four_conditions.condition == \"2-Buttony-Conversion-Buttons\") & (df_four_conditions.action == \"clic\")].shape[0]\n",
    "clickTotalC = df_four_conditions.loc[(df_four_conditions.condition == \"3-Button\") & (df_four_conditions.action == \"clic\")].shape[0]\n",
    "clickTotalD = df_four_conditions.loc[(df_four_conditions.condition == \"4-Button\") & (df_four_conditions.action == \"clic\")].shape[0]\n",
    "    \n",
    "viewTotalA = df_four_conditions.loc[(df_four_conditions.condition == \"1-Control\") & (df_four_conditions.action == \"view\")].shape[0]\n",
    "viewTotalB = df_four_conditions.loc[(df_four_conditions.condition == \"2-Buttony-Conversion-Buttons\") & (df_four_conditions.action == \"view\")].shape[0]\n",
    "viewTotalC = df_four_conditions.loc[(df_four_conditions.condition == \"3-Button\") & (df_four_conditions.action == \"view\")].shape[0]\n",
    "viewTotalD = df_four_conditions.loc[(df_four_conditions.condition == \"4-Button\") & (df_four_conditions.action == \"view\")].shape[0]\n",
    "    \n",
    "avgTotalA = clickTotalA / (clickTotalA + viewTotalA)\n",
    "avgTotalB = clickTotalB / (clickTotalB + viewTotalB)\n",
    "avgTotalC = clickTotalC / (clickTotalC + viewTotalC)\n",
    "avgTotalD = clickTotalD / (clickTotalD + viewTotalD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 53, 48, 35, 403, 413, 428, 388)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickTotalA, clickTotalB, clickTotalC, clickTotalD, viewTotalA, viewTotalB, viewTotalC, viewTotalD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_devGroup(subgroup):\n",
    "    subgroup_clickA = subgroup.loc[(subgroup.condition == \"1-Control\") & (subgroup.action == \"clic\")].shape[0]\n",
    "    subgroup_clickB = subgroup.loc[(subgroup.condition == \"2-Buttony-Conversion-Buttons\") & (subgroup.action == \"clic\")].shape[0]\n",
    "    subgroup_clickC = subgroup.loc[(subgroup.condition == \"3-Button\") & (subgroup.action == \"clic\")].shape[0]\n",
    "    subgroup_clickD = subgroup.loc[(subgroup.condition == \"4-Button\") & (subgroup.action == \"clic\")].shape[0]\n",
    "    \n",
    "    subgroup_viewA = subgroup.loc[(subgroup.condition == \"1-Control\") & (subgroup.action == \"view\")].shape[0]\n",
    "    subgroup_viewB = subgroup.loc[(subgroup.condition == \"2-Buttony-Conversion-Buttons\") & (subgroup.action == \"view\")].shape[0]\n",
    "    subgroup_viewC = subgroup.loc[(subgroup.condition == \"3-Button\") & (subgroup.action == \"view\")].shape[0]\n",
    "    subgroup_viewD = subgroup.loc[(subgroup.condition == \"4-Button\") & (subgroup.action == \"view\")].shape[0]\n",
    "    \n",
    "    zero = np.finfo(np.double).tiny # dealing with divisions of 0 (small subgroups)\n",
    "    if(subgroup_clickA == 0):\n",
    "        subgroup_clickA = zero\n",
    "    if(subgroup_clickB == 0):\n",
    "        subgroup_clickB = zero\n",
    "    if(subgroup_clickC == 0):\n",
    "        subgroup_clickC = zero\n",
    "    if(subgroup_clickD == 0):\n",
    "        subgroup_clickD = zero \n",
    "        \n",
    "    if(subgroup_viewA == 0):\n",
    "        subgroup_viewA = zero\n",
    "    if(subgroup_viewB == 0):\n",
    "        subgroup_viewB = zero\n",
    "    if(subgroup_viewC == 0):\n",
    "        subgroup_viewC = zero\n",
    "    if(subgroup_viewD == 0):\n",
    "        subgroup_viewD = zero \n",
    "    \n",
    "    subgroup_avgTotalA = subgroup_clickA / (subgroup_clickA + subgroup_viewA)\n",
    "    subgroup_avgTotalB = subgroup_clickB / (subgroup_clickB + subgroup_viewB)\n",
    "    subgroup_avgTotalC = subgroup_clickC / (subgroup_clickC + subgroup_viewC)\n",
    "    subgroup_avgTotalD = subgroup_clickD / (subgroup_clickD + subgroup_viewD)\n",
    "    \n",
    "    devVersionA = abs(subgroup_avgTotalA - avgTotalA)\n",
    "    devVersionB = abs(subgroup_avgTotalB - avgTotalB)\n",
    "    devVersionC = abs(subgroup_avgTotalC - avgTotalC)\n",
    "    devVersionD = abs(subgroup_avgTotalD - avgTotalD)\n",
    "    \n",
    "    dictGroups = {\"1-Control\": devVersionA,\n",
    "                 \"2-Buttony-Conversion-Buttons\": devVersionB,\n",
    "                 \"3-Button\": devVersionC,\n",
    "                 \"4-Button\": devVersionD}\n",
    "    max_devGroup = max(dictGroups, key = dictGroups.get) # pick the version in the subgroup with the largest target deviation\n",
    "    max_devGroupValue = dictGroups.get(max_devGroup) # the belonging value\n",
    "    return (max_devGroup, max_devGroupValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy function is almost the same as the entropy function used in question 2. The only difference is that we now should also filter the subgroup on the version with the highest targetDeviation, this will be passed to the entropy function with the second parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_function_new(subgroup, version):\n",
    "    zero = np.finfo(np.double).tiny # dealing with divisions/logarithms of 0\n",
    "    n = subgroup.loc[subgroup.condition == version].shape[0] # size subgroup in original dataset\n",
    "    N = df_four_conditions.shape[0] # size original dataset\n",
    "    n_c = N - n # complement of subgroup in orginal dataset\n",
    "    if (n == 0):\n",
    "        n = zero\n",
    "    if (n_c == 0): \n",
    "        n_c = zero\n",
    "    return ((-(n/N) * math.log2(n/N)) - ((n_c/N) * math.log2(n_c/N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the function which combines the two components. Note that we first need a call to the target deviation component. This will return a tuple which will have a value that represents the highest target deviation found among the different versions for the particular subgroup, and a string, which will encapsulate the version for which this subgroup shows the largest target deviation. This string can then be used in the entropy function, to filter the subgroup once more, as explained in detail in the previous paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxDeviationMeasure(subgroup):\n",
    "    maxTargetDev = max_devGroup(subgroup)\n",
    "    max_dev_version = maxTargetDev[0]\n",
    "    max_dev_target_dev = maxTargetDev[1]\n",
    "    entropy_max_dev = entropy_function_new(subgroup, max_dev_version)\n",
    "    quality = max_dev_target_dev * entropy_max_dev\n",
    "    return(max_dev_version, quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the results with the new quality measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pop the new quality measure into our implementation of the beam-search algorithm, while leaving all the parameters the same as in question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 2 # search depth\n",
    "w = 10 # search width (i.e. beam width)\n",
    "q = 5 # size of the set of best subgroup results (i.e. top q subgroups)\n",
    "y = 10 # number of in which numeric descriptors are dynamically discretized\n",
    "quality_measure = maxDeviationMeasure # choose between phiYule (Yule's Quality Measure), maxDeviationMeasure (max deviation measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 1\n",
      "level: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.014133866692934812,\n",
       "  [('geo_country', 'DE'),\n",
       "   ('refr_source',\n",
       "    ['StudyPortal',\n",
       "     'Yahoo',\n",
       "     'refr_source not available',\n",
       "     'Yandex',\n",
       "     'Bing',\n",
       "     'DuckDuckGo',\n",
       "     'Facebook',\n",
       "     'Vkontakte',\n",
       "     'Everyclick'])]),\n",
       " (0.014352415582794141,\n",
       "  [('refr_source',\n",
       "    ['StudyPortal',\n",
       "     'Yahoo',\n",
       "     'refr_source not available',\n",
       "     'Yandex',\n",
       "     'Bing',\n",
       "     'DuckDuckGo',\n",
       "     'Facebook',\n",
       "     'Vkontakte',\n",
       "     'Everyclick']),\n",
       "   ('os_timezone', 'Asia')]),\n",
       " (0.014357937037248403, [('geo_country', 'DE')]),\n",
       " (0.014458650143919737, [('os_timezone', 'Asia')]),\n",
       " (0.019476506118210613, [('geo_country', 'GB')])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = beam_search(d = d, w = w, q = q, quality_measure = quality_measure, df_org = df_four_conditions)\n",
    "sorted_result = priority_queue.heap_sort(result)\n",
    "sorted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some different algorithm parameters to see if we can improve the quality of the top-$q$ subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 1\n",
      "level: 2\n",
      "level: 3\n",
      "level: 4\n",
      "level: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.014133866692934812,\n",
       "  [('refr_source',\n",
       "    ['StudyPortal',\n",
       "     'Yahoo',\n",
       "     'refr_source not available',\n",
       "     'Yandex',\n",
       "     'Bing',\n",
       "     'DuckDuckGo',\n",
       "     'Facebook',\n",
       "     'Vkontakte',\n",
       "     'Everyclick']),\n",
       "   ('geo_country', 'DE')]),\n",
       " (0.014352415582794141,\n",
       "  [('refr_source',\n",
       "    ['StudyPortal',\n",
       "     'Yahoo',\n",
       "     'refr_source not available',\n",
       "     'Yandex',\n",
       "     'Bing',\n",
       "     'DuckDuckGo',\n",
       "     'Facebook',\n",
       "     'Vkontakte',\n",
       "     'Everyclick']),\n",
       "   ('os_timezone', 'Asia')]),\n",
       " (0.014357937037248403, [('geo_country', 'DE')]),\n",
       " (0.014458650143919737, [('os_timezone', 'Asia')]),\n",
       " (0.019476506118210613, [('geo_country', 'GB')])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = beam_search(d = 5, w = 20, q = 5, quality_measure = quality_measure, df_org = df_four_conditions)\n",
    "sorted_result = priority_queue.heap_sort(result)\n",
    "sorted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the depth and/or width of the beam-search leads to the same results, so let's have a look at the top-$q$ subgroups that the algorithm found. Note that half of the data used in this beam-search is artifical. The conclusions, therefore, have to be taken with a grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note that the third best found subgroup, desribed by the descriptive language $geo\\_country = DE$ is a residue of the top-$q$ found subgroups with the real data in question 2. Which makes sense since half of the data used in the implementation and evaluation of the new quality measure is the real data used in question 2. Which makes sense since you have a better chancee of finding subgroups in real data, than you do in artificially created data (affected by randomness)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are subgroups in the result of beam-search with the new quality measure. We are going to re-run the beam search with the old quality measure (Yule's quality measure), but increasing the $q$ parameter to a higher number, which will result in a list of more subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 1\n",
      "level: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.24216407978288576, [('os_timezone', 'Asia')]),\n",
       " (0.26839540764134767, [('os_timezone', 'America'), ('geo_country', 'US')]),\n",
       " (0.273346284598105, [('geo_country', 'US')]),\n",
       " (0.27697645826913331, [('geo_country', 'IN')]),\n",
       " (0.29269340845883451, [('os_timezone', 'Asia'), ('geo_country', 'IN')]),\n",
       " (0.34039077670350143, [('geo_country', 'DE')]),\n",
       " (0.3647984213664936, [('browser_language', 'latin_lan'), ('os_name', 'iOS')]),\n",
       " (0.47027223676731572,\n",
       "  [('browser_language',\n",
       "    ['greek', 'latin_lan', 'asian', 'cyrillic', 'herbrew']),\n",
       "   ('os_name', 'iOS')]),\n",
       " (0.50998200142197703, [('geo_country', 'DE'), ('os_name', 'iOS')]),\n",
       " (0.56310255294633826, [('os_name', 'iOS')])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = beam_search(d = 2, w = 10, q = 10, quality_measure = phiYule, df_org = df_action)\n",
    "sorted_result = priority_queue.heap_sort(result)\n",
    "sorted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can indeed find more subgroups which are a residue of the original dataset. The subgroup described by $os\\_timezone = 10$ is in the top-$10$ subgroups found with Yule's quality measure and the real data, and is in the top-5 found subgroups with our newly designed subgroups. The same holds for the subgroup described by $geo\\_country = DE$, which is a frequent descriptor in the top-$10$ with Yule's Quality measure, and also a frequent descriptor with our newly designed quality measure, even when half of the data is artificial. We are going to restrain ourselves from making any conclusions about subgroups desribed by attributes which were not significant in question 2 since the subgroups described by these attributes are almost certainly the result of the artificial data and, therefore, have no real implications or value for StudyPortals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the presence of descriptors which were assessed to be significant in question 2 in the beam-search results with our newly designed quality measure, we can safely conclude that this quality measure is designed and implemented properly and the EMM instance is thus able to extract additional information for when there are more than 2 versions available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
