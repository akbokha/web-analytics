{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "from queue import Queue\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>user_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>refr_source</th>\n",
       "      <th>browser_language</th>\n",
       "      <th>os_name</th>\n",
       "      <th>os_timezone</th>\n",
       "      <th>dvce_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clic</td>\n",
       "      <td>379881d5-32d7-49f4-bf5b-81fefbc5fcce</td>\n",
       "      <td>1-Control</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>greek</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clic</td>\n",
       "      <td>2a0f4218-4f62-479b-845c-109b2720e6e7</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clic</td>\n",
       "      <td>a511b6dc-2dca-455b-b5e2-bf2d224a5505</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>GB</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clic</td>\n",
       "      <td>9fb616a7-4e13-4307-ac92-0b075d7d376a</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clic</td>\n",
       "      <td>64816772-688d-4460-a591-79aa49bba0d5</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>BD</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  action                               user_id                     condition  \\\n",
       "0   clic  379881d5-32d7-49f4-bf5b-81fefbc5fcce                     1-Control   \n",
       "1   clic  2a0f4218-4f62-479b-845c-109b2720e6e7  2-Buttony-Conversion-Buttons   \n",
       "2   clic  a511b6dc-2dca-455b-b5e2-bf2d224a5505  2-Buttony-Conversion-Buttons   \n",
       "3   clic  9fb616a7-4e13-4307-ac92-0b075d7d376a  2-Buttony-Conversion-Buttons   \n",
       "4   clic  64816772-688d-4460-a591-79aa49bba0d5  2-Buttony-Conversion-Buttons   \n",
       "\n",
       "  geo_country refr_source browser_language  os_name os_timezone dvce_type  \n",
       "0          FI      Google            greek  Android      Europe    Mobile  \n",
       "1          AU      Google          english      iOS   Australia    Mobile  \n",
       "2          GB      Google          english  Android      Europe    Mobile  \n",
       "3          FI      Google          english      iOS      Europe    Mobile  \n",
       "4          BD      Google          english  Android        Asia    Mobile  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_action = pd.read_csv('data/action_condition_meta.csv')\n",
    "df_action.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We created our own priorityqueue class, because the standard priorityqueue blocks (can't insert element) when max.size is reached. We need to have a priorityqueue where the item with lowest priority is discarded and replaced by the new item.\n",
    "<i>priority_queue</i> represents min heap</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "class priority_queue:\n",
    "    def __init__(self, max_size):\n",
    "        self.items = []\n",
    "        self.max = max_size\n",
    "   \n",
    "    def push(self, item, priority):\n",
    "        if len(self.items) < self.max:\n",
    "            heapq.heappush(self.items, (priority, item))\n",
    "        else:\n",
    "            heapq.heappushpop(self.items, (priority, item))\n",
    "    \n",
    "    def get_items(self):\n",
    "        result = []\n",
    "        for i in self.items:\n",
    "            result.append(i[1])\n",
    "        return result\n",
    "\n",
    "    def pop(self):\n",
    "        return heapq.heappop(self.items)\n",
    "\n",
    "    def get_max_item(self):\n",
    "        return self.items[0]\n",
    "    \n",
    "    def empty(self):\n",
    "        return not self.items\n",
    "    \n",
    "    def print_elements(self):\n",
    "        result = []\n",
    "        for i in self.items:\n",
    "            result.append(i)\n",
    "        return result\n",
    "       \n",
    "    def heap_sort(self):\n",
    "        return [heapq.heappop(self.items) for _ in range(len(self.items))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function <i>create_dataframe</i> is used to obtain a subset of the <i>df_action</i> dataframe, based on the the description language which is passed as argument. The dataframe which is returned is the subgroup of records (users) belonging to that particular description language. For example, a group of users who all were using iOS as operating system at the time of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataframe(Set):\n",
    "    d =df_action.copy()\n",
    "    count = 0\n",
    "    for column, item in Set:\n",
    "        if count==0:\n",
    "            if isinstance(item, str): # single value\n",
    "                df_new = pd.merge(d, df_action.loc[(df_action[column] == item)], on=list(df_action), how='inner')\n",
    "            else: # list of values\n",
    "                df_new = pd.merge(d, df_action.loc[df_action[column].isin(item)], on=list(df_action), how='inner')\n",
    "            df_new.drop_duplicates(inplace = True)\n",
    "        else:\n",
    "            if isinstance(item, str): # single value\n",
    "                df_new = pd.merge(df_new, df_action.loc[(df_action[column] == item)], on=list(df_action), how='inner')\n",
    "            else: # list of values\n",
    "                df_new = pd.merge(df_new, df_action.loc[df_action[column].isin(item)], on=list(df_action), how='inner')\n",
    "        count +=1\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function <i>constraints</i> checks wheter a certain subgroup, represented by a dataframe which is passed as argument, satisfies the constraints $C$. At the moment the only constraint of a subgroup is that it should be represented by at least $5$ records (users) in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constraints(df_matches):\n",
    "    return df_matches.shape[0] > 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The refinement operator. The refinement operator gets the records and chooses to which type of attribute they belong.\n",
    "There are 3 types of attributes:\n",
    "    1. Numeric: Attribute with all number records\n",
    "    2. Binary: Attribute with true or false records\n",
    "    3. Nominal: Attribute with multiple different values in their records which are not numeric\n",
    "    \n",
    "1. For numeric values we sort all records which are in the description language D. After that we make equal-sized bins.\n",
    "The amount of bins is dependend on a predefined value. For each split point we add a description based on whether the\n",
    "numeric value is greater or equal or smaller or equal as the split point.\n",
    "2. For binary records we add one description where the description is true and one description where the description is false.\n",
    "3. For nominal values we add for each description an entry with the description and one without the description. In our case\n",
    "we take the first description in the list which is not equal to that description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_paterns_one_condition(): # level 1\n",
    "    result = []\n",
    "    columns = list(df_action.columns.values)\n",
    "    not_relevant = ['action', 'condition', 'user_id']\n",
    "    columns = [x for x in columns if (x not in not_relevant)]\n",
    "    for column in columns:\n",
    "        if df_action[column].unique().size == 2: # binary attribute\n",
    "            result.append((column, True))\n",
    "            result.append((column, False))\n",
    "        else: # nominal attribute\n",
    "            for value in df_action[column].unique():\n",
    "                all_unique = list(df_action[column].unique())\n",
    "                complement_value = [x for x in all_unique if x != value]\n",
    "                result.append((column, value))\n",
    "                result.append((column, complement_value))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor level >= 2\\ndesc_language = [] # the description language\\n\\ndef refinement(seed):\\n    unique_records = df_action[seed].unique()\\n    if unique_records.size == 0:\\n        return desc_language\\n    # no numeric attributes in dataset, so no check/implementation oft this type of attribute  \\n    if unique_records.size == 2:    #Attribute is binary\\n        desc_language.append((seed, True))\\n        desc_language.append((seed, False))\\n    else:                             #Attribute is nominal\\n        for item in unique_records:\\n            desc_language.append((seed, item))\\n            desc_language.append((seed, unique_records[next((i for i, v in enumerate(unique_records) if v != item), -1)]))\\n    return desc_language\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for level >= 2\n",
    "desc_language = [] # the description language\n",
    "\n",
    "def refinement(seed):\n",
    "    unique_records = df_action[seed].unique()\n",
    "    if unique_records.size == 0:\n",
    "        return desc_language\n",
    "    # no numeric attributes in dataset, so no check/implementation oft this type of attribute  \n",
    "    if unique_records.size == 2:    #Attribute is binary\n",
    "        desc_language.append((seed, True))\n",
    "        desc_language.append((seed, False))\n",
    "    else:                             #Attribute is nominal\n",
    "        for item in unique_records:\n",
    "            desc_language.append((seed, item))\n",
    "            desc_language.append((seed, unique_records[next((i for i, v in enumerate(unique_records) if v != item), -1)]))\n",
    "    return desc_language\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alluded in the referenced paper, the StudyPortals (original) dataset ocmes natural equiped with $m=2$ nominal targets. The first nominal target attribute is $condition$, which represents a binary column that tells us to which version the particular user was exposed during the experiment (i.e. version A or B). The second nominal target attribute is $action$, which is the binary column representing whether the page visitor merely viewed or also clicked on the button in question during the experiment. Considering these pecularities of the StudyPortals dataset, the natural choice of EMM instance would be the association model class. So we strive to find subgroups for which the association between view/click and A/B is exceptional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what model class will be exploited, the next step is to define or exploit an appropriate quality measure. Since one can easily achieve huge deviations in target behaviour (assiociation between the differences in the two nominal target attributes), it makes sense to have a dimension in the quality measure which reflects the group size. In addition one also needs to have a target deviation dimension/component in the quality measure, of course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Target Deviation Component</b> ($\\varphi_{Q}(S)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_deviation(df_matches):\n",
    "    n_1 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_2 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_3 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    n_4 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    result = (n_1*n_4 - n_2*n_3)/(n_1*n_4 + n_2*n_3)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Subgroup Size Component</b><br>\n",
    "To represent the subgroup size, we take the entropy function as described in section $3.1$ of the referenced paper. The function conceptually rewards $50/50$ splits between subgroup and complements, while punishing subgroups that are either (relatively) small or cover the vast majority of the dataset.\n",
    "\n",
    "$\\varphi_{ef}(D) = - \\frac{n}{N}lg(\\frac{n}{N}) - \\frac{n^C}{N}lg(\\frac{n^C}{N})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_function(df_matches):\n",
    "    zero = np.finfo(np.double).tiny # dealing with divisions/logarithms of 0\n",
    "    n = df_matches.shape[0] # size subgroup in original dataset\n",
    "    N = df_action.shape[0] # size original dataset\n",
    "    n_c = N - n # complement of subgroup in orginal dataset\n",
    "    if (n == 0):\n",
    "        n = zero\n",
    "    elif (n_c == 0):\n",
    "        n_c = zero\n",
    "    return ((-n/N) * math.log2(n/N)) - ((-n_c/N) * math.log2(n_c/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When combining these two components/dimensions, one obtains an association model class quality measure known as <i>Yule's Quality Measure</i>. $\\varphi_{Yule}(S) = \\varphi_{Q}(S) \\cdot \\varphi_{ef}(S) $, which boils down to a multiplication of the target deviation- and the subgroup size -component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phiYule(df_matches):\n",
    "    return target_deviation(df_matches) * entropy_function(df_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiplication of the two components ensures that subgroups are evaluated well (i.e. score well) on both components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the implementation of the $beam\\_search$ algorithm. There are a few parameters which influence the outcome, and can be changed accordingly to what one sees fit for a certain experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 1 # search depth\n",
    "w = 5 # search width (i.e. beam width)\n",
    "q = 5 # size of the set of best subgroup results (i.e. top q subgroups)\n",
    "quality_measure = phiYule # choose between phiYule (Yule's Quality Measure), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(d, w, q, quality_measure):\n",
    "    resultSet = priority_queue(q)\n",
    "    beam = priority_queue(w)\n",
    "    # first level\n",
    "    all_paterns = all_paterns_one_condition()\n",
    "    print(\"level: \"+str(1))\n",
    "    for desc in all_paterns:\n",
    "        subset_desc = create_dataframe([desc])\n",
    "        quality = quality_measure(subset_desc)\n",
    "        if (constraints(subset_desc)):\n",
    "            beam.push([desc], quality)\n",
    "            resultSet.push([desc], quality)\n",
    "    # level >= 2\n",
    "    #for level in (2, d + 1):\n",
    "     #   print(\"level: \"+str(level))\n",
    "        \n",
    "        \n",
    "    return resultSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('geo_country', 'BD')],\n",
       " [('geo_country', 'GB')],\n",
       " [('geo_country', 'US')],\n",
       " [('geo_country', 'IN')],\n",
       " [('os_timezone', 'America')]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = beam_search(d = d, w = w, q = q, quality_measure = quality_measure)\n",
    "#priority_queue.heap_sort(result)\n",
    "temp = result.get_items()\n",
    "temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('geo_country', 'BD'), ('geo_country', 'GB')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'US')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'BD'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'BD')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'US')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'GB'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'US'), ('geo_country', 'BD')],\n",
       " [('geo_country', 'US'), ('geo_country', 'GB')],\n",
       " [('geo_country', 'US'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'US'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'IN'), ('geo_country', 'BD')],\n",
       " [('geo_country', 'IN'), ('geo_country', 'GB')],\n",
       " [('geo_country', 'IN'), ('geo_country', 'US')],\n",
       " [('geo_country', 'IN'), ('os_timezone', 'America')],\n",
       " [('os_timezone', 'America'), ('geo_country', 'BD')],\n",
       " [('os_timezone', 'America'), ('geo_country', 'GB')],\n",
       " [('os_timezone', 'America'), ('geo_country', 'US')],\n",
       " [('os_timezone', 'America'), ('geo_country', 'IN')]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = temp\n",
    "result = []\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        if (i != j):\n",
    "            result.append([param[i][0], param[j][0]])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('geo_country', 'BD')],\n",
       " [('geo_country', 'GB')],\n",
       " [('geo_country', 'US')],\n",
       " [('geo_country', 'IN')],\n",
       " [('os_timezone', 'America')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refine(current_beam):\n",
    "    w = len(current_beam)\n",
    "    result= []\n",
    "    for i in range(0, w):\n",
    "        for j in range(0, w):\n",
    "            if (i != j):\n",
    "                len_i = len(current_beam[i])\n",
    "                len_j = len(current_beam[j])\n",
    "                min_len = min(len_i, len_j)\n",
    "                new = []\n",
    "                for t in (0, min_len - 1):\n",
    "                    if(current_beam[i][t] not in new):\n",
    "                        new.append(current_beam[i][t])\n",
    "                    if(current_beam[j][t] not in new):\n",
    "                        new.append(current_beam[j][t])\n",
    "                if ((list(reversed(new)) not in result) & (new not in result)):\n",
    "                    result.append(new)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('geo_country', 'BD'), ('geo_country', 'GB')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'US')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'BD'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'US')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'GB'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'US'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'US'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'IN'), ('os_timezone', 'America')]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = refine(temp)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('geo_country', 'BD'), ('geo_country', 'GB')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'US')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'BD'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'US')]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_test = test[0:5]\n",
    "slice_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('geo_country', 'BD'), ('geo_country', 'GB'), ('geo_country', 'US')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'GB'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'GB'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'US'), ('geo_country', 'GB')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'US'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'US'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'IN'), ('geo_country', 'GB')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'IN'), ('geo_country', 'US')],\n",
       " [('geo_country', 'BD'), ('geo_country', 'IN'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'BD'),\n",
       "  ('geo_country', 'GB'),\n",
       "  ('geo_country', 'IN'),\n",
       "  ('geo_country', 'US')],\n",
       " [('geo_country', 'BD'), ('os_timezone', 'America'), ('geo_country', 'GB')],\n",
       " [('geo_country', 'BD'), ('os_timezone', 'America'), ('geo_country', 'US')],\n",
       " [('geo_country', 'BD'), ('os_timezone', 'America'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'BD'),\n",
       "  ('geo_country', 'GB'),\n",
       "  ('os_timezone', 'America'),\n",
       "  ('geo_country', 'US')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'BD'), ('geo_country', 'US')],\n",
       " [('geo_country', 'GB'),\n",
       "  ('geo_country', 'BD'),\n",
       "  ('geo_country', 'US'),\n",
       "  ('geo_country', 'IN')],\n",
       " [('geo_country', 'GB'),\n",
       "  ('geo_country', 'BD'),\n",
       "  ('geo_country', 'US'),\n",
       "  ('os_timezone', 'America')]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3 = refine(slice_test)\n",
    "test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('geo_country', 'BD'), ('os_timezone', 'America'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'BD'),\n",
       "  ('geo_country', 'GB'),\n",
       "  ('os_timezone', 'America'),\n",
       "  ('geo_country', 'US')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'BD'), ('geo_country', 'US')],\n",
       " [('geo_country', 'GB'),\n",
       "  ('geo_country', 'BD'),\n",
       "  ('geo_country', 'US'),\n",
       "  ('geo_country', 'IN')],\n",
       " [('geo_country', 'GB'),\n",
       "  ('geo_country', 'BD'),\n",
       "  ('geo_country', 'US'),\n",
       "  ('os_timezone', 'America')]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_test_2 = test3[-5:]\n",
    "slice_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('geo_country', 'BD'), ('geo_country', 'IN'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'BD'),\n",
       "  ('geo_country', 'GB'),\n",
       "  ('geo_country', 'IN'),\n",
       "  ('geo_country', 'US')],\n",
       " [('geo_country', 'BD'), ('os_timezone', 'America'), ('geo_country', 'IN')],\n",
       " [('geo_country', 'BD'),\n",
       "  ('geo_country', 'GB'),\n",
       "  ('os_timezone', 'America'),\n",
       "  ('geo_country', 'US')],\n",
       " [('geo_country', 'BD'),\n",
       "  ('geo_country', 'GB'),\n",
       "  ('geo_country', 'US'),\n",
       "  ('geo_country', 'IN')],\n",
       " [('geo_country', 'BD'),\n",
       "  ('geo_country', 'GB'),\n",
       "  ('geo_country', 'US'),\n",
       "  ('os_timezone', 'America')],\n",
       " [('geo_country', 'GB'),\n",
       "  ('geo_country', 'BD'),\n",
       "  ('geo_country', 'US'),\n",
       "  ('geo_country', 'IN')],\n",
       " [('geo_country', 'GB'),\n",
       "  ('geo_country', 'BD'),\n",
       "  ('geo_country', 'US'),\n",
       "  ('os_timezone', 'America')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'US')],\n",
       " [('geo_country', 'GB'),\n",
       "  ('geo_country', 'BD'),\n",
       "  ('geo_country', 'IN'),\n",
       "  ('geo_country', 'US')],\n",
       " [('geo_country', 'GB'), ('geo_country', 'IN'), ('os_timezone', 'America')],\n",
       " [('geo_country', 'GB'),\n",
       "  ('geo_country', 'BD'),\n",
       "  ('os_timezone', 'America'),\n",
       "  ('geo_country', 'US')],\n",
       " [('geo_country', 'GB'), ('os_timezone', 'America'), ('geo_country', 'IN')]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test4 = refine(slice_test_2)\n",
    "test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    \n",
    "    for level in range(2, d + 1): # if d = depth >= 2\n",
    "        print(\"level: \"+str(level))\n",
    "        \n",
    "        while (len(candidateQueue) > 0):\n",
    "            seed = candidateQueue.pop(0)\n",
    "            set_refined = list(set(refinement(seed)))\n",
    "            for desc in set_refined:\n",
    "                df_match_condition = create_dataframe([desc])\n",
    "                quality = quality_measure(df_match_condition)\n",
    "                if constraints(df_match_condition):\n",
    "                    resultSet.push(desc, quality)\n",
    "                    beam.push(desc, quality)\n",
    "            while not beam.empty:\n",
    "                candidateQueue.append(priority_queue.pop(beam))\n",
    "    return resultSet\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
