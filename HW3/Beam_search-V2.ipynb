{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 A&B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "from queue import Queue\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') #ignore warning messages from output beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>user_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>refr_source</th>\n",
       "      <th>browser_language</th>\n",
       "      <th>os_name</th>\n",
       "      <th>os_timezone</th>\n",
       "      <th>dvce_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clic</td>\n",
       "      <td>379881d5-32d7-49f4-bf5b-81fefbc5fcce</td>\n",
       "      <td>1-Control</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>greek</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clic</td>\n",
       "      <td>2a0f4218-4f62-479b-845c-109b2720e6e7</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clic</td>\n",
       "      <td>a511b6dc-2dca-455b-b5e2-bf2d224a5505</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>GB</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clic</td>\n",
       "      <td>9fb616a7-4e13-4307-ac92-0b075d7d376a</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>FI</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clic</td>\n",
       "      <td>64816772-688d-4460-a591-79aa49bba0d5</td>\n",
       "      <td>2-Buttony-Conversion-Buttons</td>\n",
       "      <td>BD</td>\n",
       "      <td>Google</td>\n",
       "      <td>english</td>\n",
       "      <td>Android</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Mobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  action                               user_id                     condition  \\\n",
       "0   clic  379881d5-32d7-49f4-bf5b-81fefbc5fcce                     1-Control   \n",
       "1   clic  2a0f4218-4f62-479b-845c-109b2720e6e7  2-Buttony-Conversion-Buttons   \n",
       "2   clic  a511b6dc-2dca-455b-b5e2-bf2d224a5505  2-Buttony-Conversion-Buttons   \n",
       "3   clic  9fb616a7-4e13-4307-ac92-0b075d7d376a  2-Buttony-Conversion-Buttons   \n",
       "4   clic  64816772-688d-4460-a591-79aa49bba0d5  2-Buttony-Conversion-Buttons   \n",
       "\n",
       "  geo_country refr_source browser_language  os_name os_timezone dvce_type  \n",
       "0          FI      Google            greek  Android      Europe    Mobile  \n",
       "1          AU      Google          english      iOS   Australia    Mobile  \n",
       "2          GB      Google          english  Android      Europe    Mobile  \n",
       "3          FI      Google          english      iOS      Europe    Mobile  \n",
       "4          BD      Google          english  Android        Asia    Mobile  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_action = pd.read_csv('data/action_condition_meta.csv')\n",
    "df_action.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a - Beam Search implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We created our own priorityqueue class, because the standard priorityqueue blocks (can't insert element) when max.size is reached. We need to have a priorityqueue where the item with lowest priority is discarded and replaced by the new item.\n",
    "<i>priority_queue</i> represents a min heap</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alreadyInList(list1, list2):\n",
    "    permutations = list(itertools.permutations(list1))\n",
    "    for t in permutations:\n",
    "        if list(t) in list2:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "class priority_queue:\n",
    "    def __init__(self, max_size):\n",
    "        self.items = []\n",
    "        self.max = max_size\n",
    "   \n",
    "    def push(self, item, priority):\n",
    "        if ((len(self.items) < self.max) and (not alreadyInList(item, self.get_items()))):\n",
    "            heapq.heappush(self.items, (priority, item))\n",
    "        elif (not alreadyInList(item, self.get_items())):\n",
    "            heapq.heappushpop(self.items, (priority, item))\n",
    "    \n",
    "    def get_items(self):\n",
    "        result = []\n",
    "        for i in self.items:\n",
    "            result.append(i[1])\n",
    "        return result\n",
    "\n",
    "    def pop(self):\n",
    "        return heapq.heappop(self.items)\n",
    "\n",
    "    def get_max_item(self):\n",
    "        return self.items[0]\n",
    "    \n",
    "    def empty(self):\n",
    "        return not self.items\n",
    "    \n",
    "    def print_elements(self):\n",
    "        result = []\n",
    "        for i in self.items:\n",
    "            result.append(i)\n",
    "        return result\n",
    "       \n",
    "    def heap_sort(self):\n",
    "        return [heapq.heappop(self.items) for _ in range(len(self.items))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function <i>create_dataframe</i> is used to obtain a subset of the <i>df_action</i> dataframe, based on the the description language which is passed as argument. The dataframe which is returned is the subgroup of records (users) belonging to that particular description language. For example, a group of users who all were using iOS as operating system at the time of the experiment. \n",
    "\n",
    "<b>to-do (in prepocessing part?)</b>: The <i>create_dataframe</i> drops duplicate rows after each iteration. In other words, users that visited the particular version multiple times, with the same cookie-settings and the same action-type, are discarded from the dataset. It could be that a user is very fanatic in visiting the website and at every visit the user has the same action (view/clic). This can influence the dataset, because it is not fair compared to users who visit the particular version less frequently. We don't remove all the duplicated user (id's), because sometimes a user clicked on the button in a particular version, while the other time the user did not clicked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataframe(Set):\n",
    "    d =df_action.copy()\n",
    "    count = 0\n",
    "    for column, item in Set:\n",
    "        if count==0:\n",
    "            if isinstance(item, str): # single value\n",
    "                df_new = pd.merge(d, df_action.loc[(df_action[column] == item)], on=list(df_action), how='inner')\n",
    "            else: # list of values\n",
    "                df_new = pd.merge(d, df_action.loc[df_action[column].isin(item)], on=list(df_action), how='inner')\n",
    "            df_new.drop_duplicates(inplace = True)\n",
    "        else:\n",
    "            if isinstance(item, str): # single value\n",
    "                df_new = pd.merge(df_new, df_action.loc[(df_action[column] == item)], on=list(df_action), how='inner')\n",
    "            else: # list of values\n",
    "                df_new = pd.merge(df_new, df_action.loc[df_action[column].isin(item)], on=list(df_action), how='inner')\n",
    "        count +=1\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function <i>constraints</i> checks wheter a certain subgroup, represented by a dataframe which is passed as argument, satisfies the constraints $C$. At the moment the only constraint of a subgroup is that it should be represented by at least $2$% of the records (users) in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constraints(df_matches):\n",
    "    return df_matches.shape[0] > (df_action.shape[0] *0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The refinement operator gets the records and chooses to which type of attribute they belong.\n",
    "There are 3 types of attributes:\n",
    "    1. Numeric: Attribute with all number records\n",
    "    2. Binary: Attribute with true or false records\n",
    "    3. Nominal: Attribute with multiple different values in their records which are not numeric\n",
    "    \n",
    "1. For numeric values we sort all records which are in the description language D. After that we make equal-sized bins.\n",
    "The amount of bins is dependend on a predefined value. For each split point we add a description based on whether the\n",
    "numeric value is greater or equal or smaller or equal as the split point.\n",
    "2. For binary records we add one description where the description is true and one description where the description is false.\n",
    "3. For nominal values we add for each description an entry with the description and one without the description. In our case\n",
    "we take the first description in the list which is not equal to that description. \n",
    "\n",
    "For the first level, we generate all patterns consisting of <i>one</i> condition on <i>one</i> attribute. All patterns are evaluated with the quality measure $\\varphi$ and the $w$ best are saved as the <i>beam</i>. The attributes `action`, `condition` and `user_id` are discarded from the list of attributes, because these are not relevant for defining a description language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_paterns_one_condition(): # level 1\n",
    "    result = []\n",
    "    columns = list(df_action.columns.values)\n",
    "    not_relevant = ['action', 'condition', 'user_id']\n",
    "    columns = [x for x in columns if (x not in not_relevant)]\n",
    "    for column in columns:\n",
    "        if df_action[column].unique().size == 2: # binary attribute\n",
    "            result.append((column, True))\n",
    "            result.append((column, False))\n",
    "        else: # nominal attribute\n",
    "            for value in df_action[column].unique():\n",
    "                all_unique = list(df_action[column].unique())\n",
    "                complement_value = [x for x in all_unique if x != value]\n",
    "                result.append((column, value))\n",
    "                result.append((column, complement_value))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alluded in the referenced paper, the StudyPortals (original) dataset comes natural equiped with $m=2$ nominal targets. The first nominal target attribute is $condition$, which represents a binary column that tells us to which version the particular user was exposed during the experiment (i.e. version A or B). The second nominal target attribute is $action$, which is the binary column representing whether the page visitor merely viewed or also clicked on the button in question during the experiment. Considering these pecularities of the StudyPortals dataset, the natural choice of EMM instance would be the association model class. So we strive to find subgroups for which the association between view/click and A/B is exceptional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      | View | Click |\n",
    "|------|------|-------|\n",
    "|   A  |$n_1$ | $n_2$ |\n",
    "|   B  |$n_3$ | $n_4$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what model class will be exploited, the next step is to define or exploit an appropriate quality measure. Since one can easily achieve huge deviations in target behaviour (assiociation between the differences in the two nominal target attributes), it makes sense to have a dimension in the quality measure which reflects the group size. In addition one also needs to have a target deviation dimension/component in the quality measure, of course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Target Deviation Component</b> ($\\varphi_{Q}(S)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first quality measure that is implemented, is Yule's Quality Measure as described in section $4.3$ of the A&B Testing paper. For the quality measure, we use the cells of the target contingency table, given in the table above. Given a subgroup $S\\subseteq \\Omega$, we can assign each record in $S$ to the appropiate cell of this contingency table, which leads to count values for each of the $n_i$ such that: $n_1 + n_2 + n_3 + n_4 = |S|$. Yule's Q is defined as: $\\frac{(n_1\\bullet n_4 - n_2\\bullet n_3)}{(n_1\\bullet n_4 + n_2 \\bullet n_3)}$. Higher numbers on the main diagonal implies a possive assocation between the two targets and higher numbers off the main diagonal implies a negative association between the two targets. The value for $Q$ instantiated by the subgroup $S$ is denoted by $Q_S$. We contrast Yule's Q instantiated by a subgroup $S$ with Yule's Q instantiated by that subgroup complements $S^\\mathsf{C}$: $\\varphi_{Q}(S) = |Q_S - Q_{S^\\mathsf{C}} |$. This component detects subgroups whose view/click-A/B association is different from the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_deviation(df_matches):\n",
    "    zero = np.finfo(np.double).tiny\n",
    "    \n",
    "    n_1 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_2 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '1-Control')].count()[0]\n",
    "    n_3 = df_matches.loc[(df_matches.action == 'view') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    n_4 = df_matches.loc[(df_matches.action == 'clic') & (df_matches.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    Q_s = (n_1*n_4 - n_2*n_3)/(n_1*n_4 + n_2*n_3)\n",
    "    \n",
    "    df_complement = df_matches.merge(df_action, indicator=True, how='outer')\n",
    "    df_complement = df_complement[df_complement['_merge'] == 'right_only']\n",
    "    \n",
    "    n_c_1 = df_complement.loc[(df_complement.action == 'view') & (df_complement.condition == '1-Control')].count()[0]\n",
    "    n_c_2 = df_complement.loc[(df_complement.action == 'clic') & (df_complement.condition == '1-Control')].count()[0]\n",
    "    n_c_3 = df_complement.loc[(df_complement.action == 'view') & (df_complement.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    n_c_4 = df_complement.loc[(df_complement.action == 'clic') & (df_complement.condition == '2-Buttony-Conversion-Buttons')].count()[0]\n",
    "    \n",
    "    Q_s_c = (n_c_1*n_c_4 - n_c_2*n_c_3)/(n_c_1*n_c_4 + n_c_2*n_c_3)\n",
    "    \n",
    "    if (math.isnan(Q_s_c)):\n",
    "        Q_s_c = zero\n",
    "    if (math.isnan(Q_s)):\n",
    "        Q_s = zero\n",
    "        \n",
    "    phi_Q_S = abs(Q_s - Q_s_c)\n",
    "    \n",
    "    return phi_Q_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Subgroup Size Component</b><br>\n",
    "To represent the subgroup size, we take the entropy function as described in section $3.1$ of the referenced paper. The function conceptually rewards $50/50$ splits between subgroup and complements, while punishing subgroups that are either (relatively) small or cover the vast majority of the dataset.\n",
    "\n",
    "$\\varphi_{ef}(D) = - \\frac{n}{N}lg(\\frac{n}{N}) - \\frac{n^C}{N}lg(\\frac{n^C}{N})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_function(df_matches):\n",
    "    zero = np.finfo(np.double).tiny # dealing with divisions/logarithms of 0\n",
    "    n = df_matches.shape[0] # size subgroup in original dataset\n",
    "    N = df_action.shape[0] # size original dataset\n",
    "    n_c = N - n # complement of subgroup in orginal dataset\n",
    "    if (n == 0):\n",
    "        n = zero\n",
    "    if (n_c == 0): #to-do if (n_c <= 0)\n",
    "        n_c = zero\n",
    "    return ((-(n/N) * math.log2(n/N)) - ((n_c/N) * math.log2(n_c/N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When combining these two components/dimensions, one obtains an association model class quality measure known as <i>Yule's Quality Measure</i>. $\\varphi_{Yule}(S) = \\varphi_{Q}(S) \\cdot \\varphi_{ef}(S) $, which boils down to a multiplication of the target deviation- and the subgroup size -component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phiYule(df_matches):\n",
    "    return target_deviation(df_matches) * entropy_function(df_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiplication of the two components ensures that subgroups are evaluated well (i.e. score well) on both components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is responsible for the generation of new candidate patterns for the $n^{th}+1\\,level$ by refining patterns from the $n^{th}\\,level$ beam (denoted by <i>current_beam</i>). A pattern is refined into many new candidates by the conjuction of each possible single condition on a single attribute. The function results a list with all the candidates. The $beam-search$ algorithm should evaluate all $level\\,n+1$ candidates with $\\varphi$ and store the $w$ best as the new beam. In addition, it should update th list of $q-$best-performing subgroups, if there are new candidates which surpass the current top$-q$ in terms of the particular quality measure $\\varphi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refine(current_beam, d):\n",
    "    w = len(current_beam)\n",
    "    result= []\n",
    "    for i in range(0, w):\n",
    "        for j in range(0, w):\n",
    "            if (i != j):\n",
    "                len_i = len(current_beam[i])\n",
    "                len_j = len(current_beam[j])\n",
    "                min_len = min(len_i, len_j)\n",
    "                new = []\n",
    "                for t in (0, min_len - 1):\n",
    "                    if(current_beam[i][t] not in new):\n",
    "                        new.append(current_beam[i][t])\n",
    "                    if(current_beam[j][t] not in new):\n",
    "                        new.append(current_beam[j][t])\n",
    "                if ((not alreadyInList(new, current_beam)) & (len(new) == d)):\n",
    "                    result.append(new)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the implementation of the $beam\\_search$ algorithm. There are a few parameters which influence the outcome, and can be changed accordingly to what one sees fit for a certain experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 2 # search depth\n",
    "w = 5 # search width (i.e. beam width)\n",
    "q = 5 # size of the set of best subgroup results (i.e. top q subgroups)\n",
    "quality_measure = phiYule # choose between phiYule (Yule's Quality Measure), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(d, w, q, quality_measure):\n",
    "    resultSet = priority_queue(q)\n",
    "    beam = priority_queue(w)\n",
    "    # first level\n",
    "    all_paterns = all_paterns_one_condition()\n",
    "    print(\"level: \"+str(1))\n",
    "    for desc in all_paterns:\n",
    "        subset_desc = create_dataframe([desc])\n",
    "        quality = quality_measure(subset_desc)\n",
    "        if (constraints(subset_desc)):\n",
    "            beam.push([desc], quality)\n",
    "            resultSet.push([desc], quality)\n",
    "    for level in range(2, d + 1):\n",
    "        print(\"level: \"+str(level))\n",
    "        current_beam = beam.get_items()\n",
    "        new_candidates = refine(current_beam, level)\n",
    "        for c in new_candidates:\n",
    "            subset_desc = create_dataframe(c)\n",
    "            quality = quality_measure(subset_desc)\n",
    "            if (constraints(subset_desc)):\n",
    "                beam.push(c, quality)\n",
    "                resultSet.push(c, quality)\n",
    "    return resultSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 1\n",
      "level: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.70885143594824818,\n",
       "  [('refr_source',\n",
       "    ['Google',\n",
       "     'StudyPortal',\n",
       "     'refr_source not available',\n",
       "     'Yandex',\n",
       "     'Bing',\n",
       "     'DuckDuckGo',\n",
       "     'Facebook',\n",
       "     'Vkontakte',\n",
       "     'Everyclick'])]),\n",
       " (0.7349766298497219,\n",
       "  [('geo_country',\n",
       "    ['FI',\n",
       "     'AU',\n",
       "     'GB',\n",
       "     'BD',\n",
       "     'NG',\n",
       "     'EG',\n",
       "     'DE',\n",
       "     'HK',\n",
       "     'PG',\n",
       "     'SD',\n",
       "     'BR',\n",
       "     'PK',\n",
       "     'CM',\n",
       "     'GH',\n",
       "     'US',\n",
       "     'ML',\n",
       "     'RU',\n",
       "     'JP',\n",
       "     'LB',\n",
       "     'MY',\n",
       "     'IN',\n",
       "     'CA',\n",
       "     'BM',\n",
       "     'PE',\n",
       "     'ID',\n",
       "     'TZ',\n",
       "     'ZM',\n",
       "     'MM',\n",
       "     'AT',\n",
       "     'KW',\n",
       "     'PH',\n",
       "     'KZ',\n",
       "     'PY',\n",
       "     'AZ',\n",
       "     'NP',\n",
       "     'ET',\n",
       "     'IL',\n",
       "     'MX',\n",
       "     'TN',\n",
       "     'MG',\n",
       "     'ZA',\n",
       "     'IQ',\n",
       "     'SG',\n",
       "     'DO',\n",
       "     'DZ',\n",
       "     'PS',\n",
       "     'CN',\n",
       "     'AE',\n",
       "     'TH',\n",
       "     'KR',\n",
       "     'HN',\n",
       "     'GM',\n",
       "     'TW',\n",
       "     'EC',\n",
       "     'MN',\n",
       "     'VN',\n",
       "     'HT',\n",
       "     'BY',\n",
       "     'TD',\n",
       "     'UY',\n",
       "     'NI',\n",
       "     'KG',\n",
       "     'BH',\n",
       "     'KE',\n",
       "     'CO',\n",
       "     'HR',\n",
       "     'GU',\n",
       "     'RW',\n",
       "     'PA',\n",
       "     'UG'])]),\n",
       " (0.74252984893941698,\n",
       "  [('geo_country',\n",
       "    ['FI',\n",
       "     'AU',\n",
       "     'GB',\n",
       "     'BD',\n",
       "     'NG',\n",
       "     'EG',\n",
       "     'DE',\n",
       "     'HK',\n",
       "     'PG',\n",
       "     'SD',\n",
       "     'BR',\n",
       "     'PK',\n",
       "     'CM',\n",
       "     'GH',\n",
       "     'US',\n",
       "     'ML',\n",
       "     'JP',\n",
       "     'LB',\n",
       "     'MY',\n",
       "     'IN',\n",
       "     'CA',\n",
       "     'BM',\n",
       "     'PE',\n",
       "     'ID',\n",
       "     'TZ',\n",
       "     'ZM',\n",
       "     'MM',\n",
       "     'AT',\n",
       "     'KW',\n",
       "     'PH',\n",
       "     'KZ',\n",
       "     'PY',\n",
       "     'AZ',\n",
       "     'NP',\n",
       "     'ET',\n",
       "     'SA',\n",
       "     'IL',\n",
       "     'MX',\n",
       "     'TN',\n",
       "     'MG',\n",
       "     'ZA',\n",
       "     'IQ',\n",
       "     'SG',\n",
       "     'DO',\n",
       "     'DZ',\n",
       "     'PS',\n",
       "     'CN',\n",
       "     'AE',\n",
       "     'TH',\n",
       "     'KR',\n",
       "     'HN',\n",
       "     'GM',\n",
       "     'TW',\n",
       "     'EC',\n",
       "     'MN',\n",
       "     'VN',\n",
       "     'HT',\n",
       "     'BY',\n",
       "     'TD',\n",
       "     'UY',\n",
       "     'NI',\n",
       "     'KG',\n",
       "     'BH',\n",
       "     'KE',\n",
       "     'CO',\n",
       "     'HR',\n",
       "     'GU',\n",
       "     'RW',\n",
       "     'PA',\n",
       "     'UG'])]),\n",
       " (0.75355857196445108,\n",
       "  [('geo_country',\n",
       "    ['FI',\n",
       "     'AU',\n",
       "     'GB',\n",
       "     'BD',\n",
       "     'NG',\n",
       "     'EG',\n",
       "     'DE',\n",
       "     'HK',\n",
       "     'PG',\n",
       "     'SD',\n",
       "     'BR',\n",
       "     'PK',\n",
       "     'CM',\n",
       "     'GH',\n",
       "     'US',\n",
       "     'ML',\n",
       "     'RU',\n",
       "     'JP',\n",
       "     'MY',\n",
       "     'IN',\n",
       "     'CA',\n",
       "     'BM',\n",
       "     'PE',\n",
       "     'ID',\n",
       "     'TZ',\n",
       "     'ZM',\n",
       "     'MM',\n",
       "     'AT',\n",
       "     'KW',\n",
       "     'PH',\n",
       "     'KZ',\n",
       "     'PY',\n",
       "     'AZ',\n",
       "     'NP',\n",
       "     'ET',\n",
       "     'SA',\n",
       "     'IL',\n",
       "     'MX',\n",
       "     'TN',\n",
       "     'MG',\n",
       "     'ZA',\n",
       "     'IQ',\n",
       "     'SG',\n",
       "     'DO',\n",
       "     'DZ',\n",
       "     'PS',\n",
       "     'CN',\n",
       "     'AE',\n",
       "     'TH',\n",
       "     'KR',\n",
       "     'HN',\n",
       "     'GM',\n",
       "     'TW',\n",
       "     'EC',\n",
       "     'MN',\n",
       "     'VN',\n",
       "     'HT',\n",
       "     'BY',\n",
       "     'TD',\n",
       "     'UY',\n",
       "     'NI',\n",
       "     'KG',\n",
       "     'BH',\n",
       "     'KE',\n",
       "     'CO',\n",
       "     'HR',\n",
       "     'GU',\n",
       "     'RW',\n",
       "     'PA',\n",
       "     'UG'])]),\n",
       " (0.77498104352190189,\n",
       "  [('geo_country',\n",
       "    ['FI',\n",
       "     'AU',\n",
       "     'GB',\n",
       "     'BD',\n",
       "     'NG',\n",
       "     'EG',\n",
       "     'DE',\n",
       "     'HK',\n",
       "     'PG',\n",
       "     'SD',\n",
       "     'BR',\n",
       "     'CM',\n",
       "     'GH',\n",
       "     'US',\n",
       "     'ML',\n",
       "     'RU',\n",
       "     'JP',\n",
       "     'LB',\n",
       "     'MY',\n",
       "     'IN',\n",
       "     'CA',\n",
       "     'BM',\n",
       "     'PE',\n",
       "     'ID',\n",
       "     'TZ',\n",
       "     'ZM',\n",
       "     'MM',\n",
       "     'AT',\n",
       "     'KW',\n",
       "     'PH',\n",
       "     'KZ',\n",
       "     'PY',\n",
       "     'AZ',\n",
       "     'NP',\n",
       "     'ET',\n",
       "     'SA',\n",
       "     'IL',\n",
       "     'MX',\n",
       "     'TN',\n",
       "     'MG',\n",
       "     'ZA',\n",
       "     'IQ',\n",
       "     'SG',\n",
       "     'DO',\n",
       "     'DZ',\n",
       "     'PS',\n",
       "     'CN',\n",
       "     'AE',\n",
       "     'TH',\n",
       "     'KR',\n",
       "     'HN',\n",
       "     'GM',\n",
       "     'TW',\n",
       "     'EC',\n",
       "     'MN',\n",
       "     'VN',\n",
       "     'HT',\n",
       "     'BY',\n",
       "     'TD',\n",
       "     'UY',\n",
       "     'NI',\n",
       "     'KG',\n",
       "     'BH',\n",
       "     'KE',\n",
       "     'CO',\n",
       "     'HR',\n",
       "     'GU',\n",
       "     'RW',\n",
       "     'PA',\n",
       "     'UG'])])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = beam_search(d = d, w = w, q = q, quality_measure = quality_measure)\n",
    "result = priority_queue.heap_sort(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation given above allows the end user to:\n",
    "* manually set the beam width $w$ and search depth $d$\n",
    "* <b>#To-Do</b>: manually choose the number of bins in which numeric desciptors are dynamically distretized (<b>if we want to deal with numeric values!</b>)\n",
    "* <b>#To-Do</b>: easily swap out the association model class on these specific two targets for another model class (to be coded by the end user) with any number of targets of the user's choosing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b - Found Subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <i>beam search</i> algorithm is executed with parameters $d=2, w = 5$ and $q = 5$. We choose for $d=2$, i.e. a conjunction of at most 2 conditions on single descriptors, because of interpretability. When $d>2$, the results become more complex and therefore give less information on which a domain expert can act. With $q = 5$, the output of <i> beam search</i> can be easily compared to the top five subgroups found in the A&B Testing paper. After experimenting with the paramaters, we found out that the search width $w$ is hard to set manually and has a big influence on the outcome of subgroups. We choose for $w=10$, for the same reason when we choose for $q=5$. The <i>beam search</i> is therefore executed with the same parameters as the referenced A/B Testing paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset $\\Omega$ that is preprocessed in Q1 has a total number of 899 records. Yule's Q has a value of $\\varphi_Q(\\Omega) = 0.27$. In other words, the result of the traditional A/B test tells us that variant B: the more buttony variantion generates more clicks than the less buttony control version. The new variation is therefore slightly better, it can be argued whether the difference is significant. In A&B Testing, we can mine deeper into the data and find specific subgroups which prefer version A or version B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>#to-do</b>: describe top five subgroups in detail \n",
    "* <b>#to-do</b>: compare found subgroups to subgroups in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>0.708851435948</td><td>[('refr_source', ['Google', 'StudyPortal', 'refr_source not available', 'Yandex', 'Bing', 'DuckDuckGo', 'Facebook', 'Vkontakte', 'Everyclick'])]</td></tr><tr><td>0.73497662985</td><td>[('geo_country', ['FI', 'AU', 'GB', 'BD', 'NG', 'EG', 'DE', 'HK', 'PG', 'SD', 'BR', 'PK', 'CM', 'GH', 'US', 'ML', 'RU', 'JP', 'LB', 'MY', 'IN', 'CA', 'BM', 'PE', 'ID', 'TZ', 'ZM', 'MM', 'AT', 'KW', 'PH', 'KZ', 'PY', 'AZ', 'NP', 'ET', 'IL', 'MX', 'TN', 'MG', 'ZA', 'IQ', 'SG', 'DO', 'DZ', 'PS', 'CN', 'AE', 'TH', 'KR', 'HN', 'GM', 'TW', 'EC', 'MN', 'VN', 'HT', 'BY', 'TD', 'UY', 'NI', 'KG', 'BH', 'KE', 'CO', 'HR', 'GU', 'RW', 'PA', 'UG'])]</td></tr><tr><td>0.742529848939</td><td>[('geo_country', ['FI', 'AU', 'GB', 'BD', 'NG', 'EG', 'DE', 'HK', 'PG', 'SD', 'BR', 'PK', 'CM', 'GH', 'US', 'ML', 'JP', 'LB', 'MY', 'IN', 'CA', 'BM', 'PE', 'ID', 'TZ', 'ZM', 'MM', 'AT', 'KW', 'PH', 'KZ', 'PY', 'AZ', 'NP', 'ET', 'SA', 'IL', 'MX', 'TN', 'MG', 'ZA', 'IQ', 'SG', 'DO', 'DZ', 'PS', 'CN', 'AE', 'TH', 'KR', 'HN', 'GM', 'TW', 'EC', 'MN', 'VN', 'HT', 'BY', 'TD', 'UY', 'NI', 'KG', 'BH', 'KE', 'CO', 'HR', 'GU', 'RW', 'PA', 'UG'])]</td></tr><tr><td>0.753558571964</td><td>[('geo_country', ['FI', 'AU', 'GB', 'BD', 'NG', 'EG', 'DE', 'HK', 'PG', 'SD', 'BR', 'PK', 'CM', 'GH', 'US', 'ML', 'RU', 'JP', 'MY', 'IN', 'CA', 'BM', 'PE', 'ID', 'TZ', 'ZM', 'MM', 'AT', 'KW', 'PH', 'KZ', 'PY', 'AZ', 'NP', 'ET', 'SA', 'IL', 'MX', 'TN', 'MG', 'ZA', 'IQ', 'SG', 'DO', 'DZ', 'PS', 'CN', 'AE', 'TH', 'KR', 'HN', 'GM', 'TW', 'EC', 'MN', 'VN', 'HT', 'BY', 'TD', 'UY', 'NI', 'KG', 'BH', 'KE', 'CO', 'HR', 'GU', 'RW', 'PA', 'UG'])]</td></tr><tr><td>0.774981043522</td><td>[('geo_country', ['FI', 'AU', 'GB', 'BD', 'NG', 'EG', 'DE', 'HK', 'PG', 'SD', 'BR', 'CM', 'GH', 'US', 'ML', 'RU', 'JP', 'LB', 'MY', 'IN', 'CA', 'BM', 'PE', 'ID', 'TZ', 'ZM', 'MM', 'AT', 'KW', 'PH', 'KZ', 'PY', 'AZ', 'NP', 'ET', 'SA', 'IL', 'MX', 'TN', 'MG', 'ZA', 'IQ', 'SG', 'DO', 'DZ', 'PS', 'CN', 'AE', 'TH', 'KR', 'HN', 'GM', 'TW', 'EC', 'MN', 'VN', 'HT', 'BY', 'TD', 'UY', 'NI', 'KG', 'BH', 'KE', 'CO', 'HR', 'GU', 'RW', 'PA', 'UG'])]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "display(HTML(\n",
    "    '<table><tr>{}</tr></table>'.format(\n",
    "        '</tr><tr>'.join(\n",
    "            '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in result)\n",
    "        )\n",
    " ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
